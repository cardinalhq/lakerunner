// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.29.0
// source: inqueue_claim_batch.sql

package lrdb

import (
	"context"
	"time"

	"github.com/google/uuid"
)

const claimInqueueWorkBatch = `-- name: ClaimInqueueWorkBatch :many
WITH
params AS (
  SELECT
    $1::uuid     AS organization_id,
    $2::smallint    AS instance_num,
    $3::text      AS telemetry_type,
    $4::bigint         AS worker_id,
    COALESCE($5::timestamptz, now()) AS now_ts,
    $6::bigint    AS max_total_size,    -- hard cap
    $7::bigint    AS min_total_size,    -- fresh-path minimum-by-size
    $8::integer  AS max_age_seconds,
    $9::integer      AS batch_count        -- row cap (no more than)
),
scope AS (
  SELECT q.id, q.queue_ts, q.priority, q.organization_id, q.collector_name, q.instance_num, q.bucket, q.object_id, q.telemetry_type, q.tries, q.claimed_by, q.claimed_at, q.file_size
  FROM inqueue q
  JOIN params p ON TRUE
  WHERE q.claimed_at IS NULL
    AND q.organization_id = p.organization_id
    AND q.instance_num    = p.instance_num
    AND q.telemetry_type  = p.telemetry_type
  ORDER BY q.priority DESC, q.queue_ts ASC
  FOR UPDATE SKIP LOCKED
),
oldest AS (
  SELECT id, queue_ts, priority, organization_id, collector_name, instance_num, bucket, object_id, telemetry_type, tries, claimed_by, claimed_at, file_size FROM scope LIMIT 1
),
flags AS (
  SELECT
    (o.file_size >  p.max_total_size)                               AS is_oversized,
    ((p.now_ts - o.queue_ts) > make_interval(secs => p.max_age_seconds)) AS is_old,
    p.organization_id, p.instance_num, p.telemetry_type, p.worker_id, p.now_ts, p.max_total_size, p.min_total_size, p.max_age_seconds, p.batch_count
  FROM params p
  JOIN oldest o ON TRUE
),
pack AS (
  SELECT
    s.id, s.queue_ts, s.priority, s.organization_id, s.collector_name, s.instance_num, s.bucket, s.object_id, s.telemetry_type, s.tries, s.claimed_by, s.claimed_at, s.file_size,
    SUM(s.file_size) OVER (ORDER BY s.priority DESC, s.queue_ts ASC
                           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cum_size,
    ROW_NUMBER() OVER (ORDER BY s.priority DESC, s.queue_ts ASC)              AS rn
  FROM scope s
),
prelim AS (
  SELECT p.id, p.queue_ts, p.priority, p.organization_id, p.collector_name, p.instance_num, p.bucket, p.object_id, p.telemetry_type, p.tries, p.claimed_by, p.claimed_at, p.file_size, p.cum_size, p.rn
  FROM pack p
  JOIN flags f ON TRUE
  WHERE p.cum_size <= f.max_total_size
    AND p.rn      <= f.batch_count
),
prelim_stats AS (
  SELECT
    COUNT(*) AS n_rows,
    COALESCE(SUM(file_size), 0) AS total_size
  FROM prelim
),
chosen AS (
  -- 1) Oversized oldest -> claim just that one
  (
    SELECT o.id, o.queue_ts, o.priority, o.organization_id, o.collector_name, o.instance_num, o.bucket, o.object_id, o.telemetry_type, o.tries, o.claimed_by, o.claimed_at, o.file_size
    FROM oldest o
    JOIN flags f ON TRUE
    WHERE f.is_oversized
  )
  UNION ALL
  -- 2) Too old -> take whatever fits under caps (ignore min size)
  (
    SELECT p.id, p.queue_ts, p.priority, p.organization_id, p.collector_name, p.instance_num, p.bucket, p.object_id, p.telemetry_type, p.tries, p.claimed_by, p.claimed_at, p.file_size, p.cum_size, p.rn
    FROM prelim p
    JOIN flags f ON TRUE
    WHERE NOT f.is_oversized AND f.is_old
  )
  UNION ALL
  -- 3) Fresh -> only if size minimum met (and caps already enforced by prelim)
  (
    SELECT p.id, p.queue_ts, p.priority, p.organization_id, p.collector_name, p.instance_num, p.bucket, p.object_id, p.telemetry_type, p.tries, p.claimed_by, p.claimed_at, p.file_size, p.cum_size, p.rn
    FROM prelim p
    JOIN flags f ON TRUE
    JOIN prelim_stats ps ON TRUE
    WHERE NOT f.is_oversized
      AND NOT f.is_old
      AND ps.total_size >= f.min_total_size
  )
),
upd AS (
  UPDATE inqueue q
  SET claimed_by = (SELECT worker_id FROM params),
      claimed_at = (SELECT now_ts FROM params)
  FROM chosen c
  WHERE q.id = c.id
  RETURNING q.id, q.queue_ts, q.priority, q.organization_id, q.collector_name, q.instance_num, q.bucket, q.object_id, q.telemetry_type, q.tries, q.claimed_by, q.claimed_at, q.file_size
)
SELECT id, queue_ts, priority, organization_id, collector_name, instance_num, bucket, object_id, telemetry_type, tries, claimed_by, claimed_at, file_size FROM upd
ORDER BY priority DESC, queue_ts ASC
`

type ClaimInqueueWorkBatchParams struct {
	OrganizationID uuid.UUID  `json:"organization_id"`
	InstanceNum    int16      `json:"instance_num"`
	TelemetryType  string     `json:"telemetry_type"`
	WorkerID       int64      `json:"worker_id"`
	NowTs          *time.Time `json:"now_ts"`
	MaxTotalSize   int64      `json:"max_total_size"`
	MinTotalSize   int64      `json:"min_total_size"`
	MaxAgeSeconds  int32      `json:"max_age_seconds"`
	BatchCount     int32      `json:"batch_count"`
}

type ClaimInqueueWorkBatchRow struct {
	ID             uuid.UUID  `json:"id"`
	QueueTs        time.Time  `json:"queue_ts"`
	Priority       int32      `json:"priority"`
	OrganizationID uuid.UUID  `json:"organization_id"`
	CollectorName  string     `json:"collector_name"`
	InstanceNum    int16      `json:"instance_num"`
	Bucket         string     `json:"bucket"`
	ObjectID       string     `json:"object_id"`
	TelemetryType  string     `json:"telemetry_type"`
	Tries          int32      `json:"tries"`
	ClaimedBy      int64      `json:"claimed_by"`
	ClaimedAt      *time.Time `json:"claimed_at"`
	FileSize       int64      `json:"file_size"`
}

// Greedy pack up to size cap and row cap
func (q *Queries) ClaimInqueueWorkBatch(ctx context.Context, arg ClaimInqueueWorkBatchParams) ([]ClaimInqueueWorkBatchRow, error) {
	rows, err := q.db.Query(ctx, claimInqueueWorkBatch,
		arg.OrganizationID,
		arg.InstanceNum,
		arg.TelemetryType,
		arg.WorkerID,
		arg.NowTs,
		arg.MaxTotalSize,
		arg.MinTotalSize,
		arg.MaxAgeSeconds,
		arg.BatchCount,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ClaimInqueueWorkBatchRow
	for rows.Next() {
		var i ClaimInqueueWorkBatchRow
		if err := rows.Scan(
			&i.ID,
			&i.QueueTs,
			&i.Priority,
			&i.OrganizationID,
			&i.CollectorName,
			&i.InstanceNum,
			&i.Bucket,
			&i.ObjectID,
			&i.TelemetryType,
			&i.Tries,
			&i.ClaimedBy,
			&i.ClaimedAt,
			&i.FileSize,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}
