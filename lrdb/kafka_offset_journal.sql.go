// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.29.0
// source: kafka_offset_journal.sql

package lrdb

import (
	"context"
	"time"
)

const deleteOldKafkaOffsets = `-- name: DeleteOldKafkaOffsets :exec
DELETE FROM kafka_offset_journal 
WHERE updated_at < $1
`

// Clean up old offset entries (older than specified timestamp)
func (q *Queries) DeleteOldKafkaOffsets(ctx context.Context, cutoffTime time.Time) error {
	_, err := q.db.Exec(ctx, deleteOldKafkaOffsets, cutoffTime)
	return err
}

const getKafkaOffsetsByConsumerGroup = `-- name: GetKafkaOffsetsByConsumerGroup :many
SELECT consumer_group, topic, partition, last_processed_offset, updated_at
FROM kafka_offset_journal 
WHERE consumer_group = $1
ORDER BY topic, partition
`

// Get all offset entries for a specific consumer group (useful for monitoring)
func (q *Queries) GetKafkaOffsetsByConsumerGroup(ctx context.Context, consumerGroup string) ([]KafkaOffsetJournal, error) {
	rows, err := q.db.Query(ctx, getKafkaOffsetsByConsumerGroup, consumerGroup)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []KafkaOffsetJournal
	for rows.Next() {
		var i KafkaOffsetJournal
		if err := rows.Scan(
			&i.ConsumerGroup,
			&i.Topic,
			&i.Partition,
			&i.LastProcessedOffset,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const kafkaJournalGetLastProcessed = `-- name: KafkaJournalGetLastProcessed :one
SELECT last_processed_offset 
FROM kafka_offset_journal 
WHERE consumer_group = $1 AND topic = $2 AND partition = $3
`

type KafkaJournalGetLastProcessedParams struct {
	ConsumerGroup string `json:"consumer_group"`
	Topic         string `json:"topic"`
	Partition     int32  `json:"partition"`
}

// Get the last processed offset for a specific consumer group, topic, and partition
func (q *Queries) KafkaJournalGetLastProcessed(ctx context.Context, arg KafkaJournalGetLastProcessedParams) (int64, error) {
	row := q.db.QueryRow(ctx, kafkaJournalGetLastProcessed, arg.ConsumerGroup, arg.Topic, arg.Partition)
	var last_processed_offset int64
	err := row.Scan(&last_processed_offset)
	return last_processed_offset, err
}

const kafkaJournalUpsert = `-- name: KafkaJournalUpsert :exec
INSERT INTO kafka_offset_journal (consumer_group, topic, partition, last_processed_offset, updated_at)
VALUES ($1, $2, $3, $4, NOW())
ON CONFLICT (consumer_group, topic, partition)
DO UPDATE SET 
    last_processed_offset = EXCLUDED.last_processed_offset,
    updated_at = NOW()
WHERE kafka_offset_journal.last_processed_offset < EXCLUDED.last_processed_offset
`

type KafkaJournalUpsertParams struct {
	ConsumerGroup       string `json:"consumer_group"`
	Topic               string `json:"topic"`
	Partition           int32  `json:"partition"`
	LastProcessedOffset int64  `json:"last_processed_offset"`
}

// Insert or update the last processed offset for a consumer group, topic, and partition
// Only updates if the new offset is greater than the existing one
func (q *Queries) KafkaJournalUpsert(ctx context.Context, arg KafkaJournalUpsertParams) error {
	_, err := q.db.Exec(ctx, kafkaJournalUpsert,
		arg.ConsumerGroup,
		arg.Topic,
		arg.Partition,
		arg.LastProcessedOffset,
	)
	return err
}
