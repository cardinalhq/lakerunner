// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.29.0
// source: batch.go

package lrdb

import (
	"context"
	"errors"

	"github.com/google/uuid"
	"github.com/jackc/pgx/v5"
)

var (
	ErrBatchAlreadyClosed = errors.New("batch already closed")
)

const batchDeleteMetricSegs = `-- name: BatchDeleteMetricSegs :batchexec
DELETE FROM public.metric_seg
 WHERE organization_id = $1
   AND dateint         = $2
   AND frequency_ms    = $3
   AND segment_id      = $4
   AND instance_num    = $5
   AND slot_id         = $6
`

type BatchDeleteMetricSegsBatchResults struct {
	br     pgx.BatchResults
	tot    int
	closed bool
}

type BatchDeleteMetricSegsParams struct {
	OrganizationID uuid.UUID `json:"organization_id"`
	Dateint        int32     `json:"dateint"`
	FrequencyMs    int32     `json:"frequency_ms"`
	SegmentID      int64     `json:"segment_id"`
	InstanceNum    int16     `json:"instance_num"`
	SlotID         int32     `json:"slot_id"`
}

func (q *Queries) BatchDeleteMetricSegs(ctx context.Context, arg []BatchDeleteMetricSegsParams) *BatchDeleteMetricSegsBatchResults {
	batch := &pgx.Batch{}
	for _, a := range arg {
		vals := []interface{}{
			a.OrganizationID,
			a.Dateint,
			a.FrequencyMs,
			a.SegmentID,
			a.InstanceNum,
			a.SlotID,
		}
		batch.Queue(batchDeleteMetricSegs, vals...)
	}
	br := q.db.SendBatch(ctx, batch)
	return &BatchDeleteMetricSegsBatchResults{br, len(arg), false}
}

func (b *BatchDeleteMetricSegsBatchResults) Exec(f func(int, error)) {
	defer b.br.Close()
	for t := 0; t < b.tot; t++ {
		if b.closed {
			if f != nil {
				f(t, ErrBatchAlreadyClosed)
			}
			continue
		}
		_, err := b.br.Exec()
		if f != nil {
			f(t, err)
		}
	}
}

func (b *BatchDeleteMetricSegsBatchResults) Close() error {
	b.closed = true
	return b.br.Close()
}

const batchInsertLogSegs = `-- name: BatchInsertLogSegs :batchexec
INSERT INTO log_seg (
  organization_id,
  dateint,
  ingest_dateint,
  segment_id,
  instance_num,
  slot_id,
  ts_range,
  record_count,
  file_size,
  created_by,
  fingerprints
)
VALUES (
  $1,
  $2,
  $3,
  $4,
  $5,
  $6,
  int8range($7, $8, '[)'),
  $9,
  $10,
  $11,
  $12::bigint[]
)
`

type BatchInsertLogSegsBatchResults struct {
	br     pgx.BatchResults
	tot    int
	closed bool
}

type BatchInsertLogSegsParams struct {
	OrganizationID uuid.UUID `json:"organization_id"`
	Dateint        int32     `json:"dateint"`
	IngestDateint  int32     `json:"ingest_dateint"`
	SegmentID      int64     `json:"segment_id"`
	InstanceNum    int16     `json:"instance_num"`
	SlotID         int32     `json:"slot_id"`
	StartTs        int64     `json:"start_ts"`
	EndTs          int64     `json:"end_ts"`
	RecordCount    int64     `json:"record_count"`
	FileSize       int64     `json:"file_size"`
	CreatedBy      CreatedBy `json:"created_by"`
	Fingerprints   []int64   `json:"fingerprints"`
}

func (q *Queries) BatchInsertLogSegs(ctx context.Context, arg []BatchInsertLogSegsParams) *BatchInsertLogSegsBatchResults {
	batch := &pgx.Batch{}
	for _, a := range arg {
		vals := []interface{}{
			a.OrganizationID,
			a.Dateint,
			a.IngestDateint,
			a.SegmentID,
			a.InstanceNum,
			a.SlotID,
			a.StartTs,
			a.EndTs,
			a.RecordCount,
			a.FileSize,
			a.CreatedBy,
			a.Fingerprints,
		}
		batch.Queue(batchInsertLogSegs, vals...)
	}
	br := q.db.SendBatch(ctx, batch)
	return &BatchInsertLogSegsBatchResults{br, len(arg), false}
}

func (b *BatchInsertLogSegsBatchResults) Exec(f func(int, error)) {
	defer b.br.Close()
	for t := 0; t < b.tot; t++ {
		if b.closed {
			if f != nil {
				f(t, ErrBatchAlreadyClosed)
			}
			continue
		}
		_, err := b.br.Exec()
		if f != nil {
			f(t, err)
		}
	}
}

func (b *BatchInsertLogSegsBatchResults) Close() error {
	b.closed = true
	return b.br.Close()
}

const batchInsertMetricSegs = `-- name: BatchInsertMetricSegs :batchexec
INSERT INTO metric_seg (
  organization_id,
  dateint,
  ingest_dateint,
  frequency_ms,
  segment_id,
  instance_num,
  slot_id,
  ts_range,
  record_count,
  file_size,
  published,
  created_by,
  rolledup,
  fingerprints,
  sort_version,
  slot_count,
  compacted
)
VALUES (
  $1,
  $2,
  $3,
  $4,
  $5,
  $6,
  $7,
  int8range($8, $9, '[)'),
  $10,
  $11,
  $12,
  $13,
  $14,
  $15::bigint[],
  $16,
  $17,
  $18
)
`

type BatchInsertMetricSegsBatchResults struct {
	br     pgx.BatchResults
	tot    int
	closed bool
}

type BatchInsertMetricSegsParams struct {
	OrganizationID uuid.UUID `json:"organization_id"`
	Dateint        int32     `json:"dateint"`
	IngestDateint  int32     `json:"ingest_dateint"`
	FrequencyMs    int32     `json:"frequency_ms"`
	SegmentID      int64     `json:"segment_id"`
	InstanceNum    int16     `json:"instance_num"`
	SlotID         int32     `json:"slot_id"`
	StartTs        int64     `json:"start_ts"`
	EndTs          int64     `json:"end_ts"`
	RecordCount    int64     `json:"record_count"`
	FileSize       int64     `json:"file_size"`
	Published      bool      `json:"published"`
	CreatedBy      CreatedBy `json:"created_by"`
	Rolledup       bool      `json:"rolledup"`
	Fingerprints   []int64   `json:"fingerprints"`
	SortVersion    int16     `json:"sort_version"`
	SlotCount      int32     `json:"slot_count"`
	Compacted      bool      `json:"compacted"`
}

func (q *Queries) BatchInsertMetricSegs(ctx context.Context, arg []BatchInsertMetricSegsParams) *BatchInsertMetricSegsBatchResults {
	batch := &pgx.Batch{}
	for _, a := range arg {
		vals := []interface{}{
			a.OrganizationID,
			a.Dateint,
			a.IngestDateint,
			a.FrequencyMs,
			a.SegmentID,
			a.InstanceNum,
			a.SlotID,
			a.StartTs,
			a.EndTs,
			a.RecordCount,
			a.FileSize,
			a.Published,
			a.CreatedBy,
			a.Rolledup,
			a.Fingerprints,
			a.SortVersion,
			a.SlotCount,
			a.Compacted,
		}
		batch.Queue(batchInsertMetricSegs, vals...)
	}
	br := q.db.SendBatch(ctx, batch)
	return &BatchInsertMetricSegsBatchResults{br, len(arg), false}
}

func (b *BatchInsertMetricSegsBatchResults) Exec(f func(int, error)) {
	defer b.br.Close()
	for t := 0; t < b.tot; t++ {
		if b.closed {
			if f != nil {
				f(t, ErrBatchAlreadyClosed)
			}
			continue
		}
		_, err := b.br.Exec()
		if f != nil {
			f(t, err)
		}
	}
}

func (b *BatchInsertMetricSegsBatchResults) Close() error {
	b.closed = true
	return b.br.Close()
}

const batchInsertTraceSegs = `-- name: BatchInsertTraceSegs :batchexec
INSERT INTO trace_seg (
  organization_id,
  dateint,
  ingest_dateint,
  segment_id,
  instance_num,
  slot_id,
  ts_range,
  record_count,
  file_size,
  created_by,
  fingerprints
)
VALUES (
  $1,
  $2,
  $3,
  $4,
  $5,
  $6,
  int8range($7, $8, '[)'),
  $9,
  $10,
  $11,
  $12::bigint[]
)
`

type BatchInsertTraceSegsBatchResults struct {
	br     pgx.BatchResults
	tot    int
	closed bool
}

type BatchInsertTraceSegsParams struct {
	OrganizationID uuid.UUID `json:"organization_id"`
	Dateint        int32     `json:"dateint"`
	IngestDateint  int32     `json:"ingest_dateint"`
	SegmentID      int64     `json:"segment_id"`
	InstanceNum    int16     `json:"instance_num"`
	SlotID         int32     `json:"slot_id"`
	StartTs        int64     `json:"start_ts"`
	EndTs          int64     `json:"end_ts"`
	RecordCount    int64     `json:"record_count"`
	FileSize       int64     `json:"file_size"`
	CreatedBy      CreatedBy `json:"created_by"`
	Fingerprints   []int64   `json:"fingerprints"`
}

func (q *Queries) BatchInsertTraceSegs(ctx context.Context, arg []BatchInsertTraceSegsParams) *BatchInsertTraceSegsBatchResults {
	batch := &pgx.Batch{}
	for _, a := range arg {
		vals := []interface{}{
			a.OrganizationID,
			a.Dateint,
			a.IngestDateint,
			a.SegmentID,
			a.InstanceNum,
			a.SlotID,
			a.StartTs,
			a.EndTs,
			a.RecordCount,
			a.FileSize,
			a.CreatedBy,
			a.Fingerprints,
		}
		batch.Queue(batchInsertTraceSegs, vals...)
	}
	br := q.db.SendBatch(ctx, batch)
	return &BatchInsertTraceSegsBatchResults{br, len(arg), false}
}

func (b *BatchInsertTraceSegsBatchResults) Exec(f func(int, error)) {
	defer b.br.Close()
	for t := 0; t < b.tot; t++ {
		if b.closed {
			if f != nil {
				f(t, ErrBatchAlreadyClosed)
			}
			continue
		}
		_, err := b.br.Exec()
		if f != nil {
			f(t, err)
		}
	}
}

func (b *BatchInsertTraceSegsBatchResults) Close() error {
	b.closed = true
	return b.br.Close()
}

const batchMarkMetricSegsRolledup = `-- name: BatchMarkMetricSegsRolledup :batchexec
UPDATE public.metric_seg
   SET rolledup = true
 WHERE organization_id = $1
   AND dateint         = $2
   AND frequency_ms    = $3
   AND segment_id      = $4
   AND instance_num    = $5
   AND slot_id         = $6
`

type BatchMarkMetricSegsRolledupBatchResults struct {
	br     pgx.BatchResults
	tot    int
	closed bool
}

type BatchMarkMetricSegsRolledupParams struct {
	OrganizationID uuid.UUID `json:"organization_id"`
	Dateint        int32     `json:"dateint"`
	FrequencyMs    int32     `json:"frequency_ms"`
	SegmentID      int64     `json:"segment_id"`
	InstanceNum    int16     `json:"instance_num"`
	SlotID         int32     `json:"slot_id"`
}

func (q *Queries) BatchMarkMetricSegsRolledup(ctx context.Context, arg []BatchMarkMetricSegsRolledupParams) *BatchMarkMetricSegsRolledupBatchResults {
	batch := &pgx.Batch{}
	for _, a := range arg {
		vals := []interface{}{
			a.OrganizationID,
			a.Dateint,
			a.FrequencyMs,
			a.SegmentID,
			a.InstanceNum,
			a.SlotID,
		}
		batch.Queue(batchMarkMetricSegsRolledup, vals...)
	}
	br := q.db.SendBatch(ctx, batch)
	return &BatchMarkMetricSegsRolledupBatchResults{br, len(arg), false}
}

func (b *BatchMarkMetricSegsRolledupBatchResults) Exec(f func(int, error)) {
	defer b.br.Close()
	for t := 0; t < b.tot; t++ {
		if b.closed {
			if f != nil {
				f(t, ErrBatchAlreadyClosed)
			}
			continue
		}
		_, err := b.br.Exec()
		if f != nil {
			f(t, err)
		}
	}
}

func (b *BatchMarkMetricSegsRolledupBatchResults) Close() error {
	b.closed = true
	return b.br.Close()
}

const batchUpsertExemplarLogs = `-- name: BatchUpsertExemplarLogs :batchone
INSERT INTO lrdb_exemplar_logs
            ( organization_id,  service_identifier_id,  fingerprint,  attributes,  exemplar)
VALUES      ($1, $2, $3, $4, $5)
ON CONFLICT ( organization_id,  service_identifier_id,  fingerprint)
DO UPDATE SET
  attributes = EXCLUDED.attributes,
  exemplar   = EXCLUDED.exemplar,
  updated_at = now(),
  related_fingerprints = CASE
    WHEN $6::BIGINT != 0
      AND $3 != $6
      THEN add_to_bigint_list(lrdb_exemplar_logs.related_fingerprints, $6, 100)
    ELSE lrdb_exemplar_logs.related_fingerprints
  END
RETURNING (created_at = updated_at) as is_new
`

type BatchUpsertExemplarLogsBatchResults struct {
	br     pgx.BatchResults
	tot    int
	closed bool
}

type BatchUpsertExemplarLogsParams struct {
	OrganizationID      uuid.UUID      `json:"organization_id"`
	ServiceIdentifierID uuid.UUID      `json:"service_identifier_id"`
	Fingerprint         int64          `json:"fingerprint"`
	Attributes          map[string]any `json:"attributes"`
	Exemplar            map[string]any `json:"exemplar"`
	OldFingerprint      int64          `json:"old_fingerprint"`
}

// This will upsert a new log exemplar. Attributes, exemplar, and updated_at are always updated
// to the provided values. If old_fingerprint is not 0, it is added to the list of related
// fingerprints. This means the "old" fingerprint should be fingerprint, so it always updates
// an existing record, not changing it to the new one.
// The return value is a boolean indicating if the record is new.
func (q *Queries) BatchUpsertExemplarLogs(ctx context.Context, arg []BatchUpsertExemplarLogsParams) *BatchUpsertExemplarLogsBatchResults {
	batch := &pgx.Batch{}
	for _, a := range arg {
		vals := []interface{}{
			a.OrganizationID,
			a.ServiceIdentifierID,
			a.Fingerprint,
			a.Attributes,
			a.Exemplar,
			a.OldFingerprint,
		}
		batch.Queue(batchUpsertExemplarLogs, vals...)
	}
	br := q.db.SendBatch(ctx, batch)
	return &BatchUpsertExemplarLogsBatchResults{br, len(arg), false}
}

func (b *BatchUpsertExemplarLogsBatchResults) QueryRow(f func(int, bool, error)) {
	defer b.br.Close()
	for t := 0; t < b.tot; t++ {
		var is_new bool
		if b.closed {
			if f != nil {
				f(t, is_new, ErrBatchAlreadyClosed)
			}
			continue
		}
		row := b.br.QueryRow()
		err := row.Scan(&is_new)
		if f != nil {
			f(t, is_new, err)
		}
	}
}

func (b *BatchUpsertExemplarLogsBatchResults) Close() error {
	b.closed = true
	return b.br.Close()
}

const batchUpsertExemplarMetrics = `-- name: BatchUpsertExemplarMetrics :batchone
INSERT INTO lrdb_exemplar_metrics
            ( organization_id,  service_identifier_id,  metric_name,  metric_type,  attributes,  exemplar)
VALUES      ($1, $2, $3, $4, $5, $6)
ON CONFLICT ( organization_id,  service_identifier_id,  metric_name,  metric_type)
DO UPDATE SET
  attributes = EXCLUDED.attributes,
  exemplar = EXCLUDED.exemplar,
  updated_at = now()
RETURNING (created_at = updated_at) as is_new
`

type BatchUpsertExemplarMetricsBatchResults struct {
	br     pgx.BatchResults
	tot    int
	closed bool
}

type BatchUpsertExemplarMetricsParams struct {
	OrganizationID      uuid.UUID      `json:"organization_id"`
	ServiceIdentifierID uuid.UUID      `json:"service_identifier_id"`
	MetricName          string         `json:"metric_name"`
	MetricType          string         `json:"metric_type"`
	Attributes          map[string]any `json:"attributes"`
	Exemplar            map[string]any `json:"exemplar"`
}

func (q *Queries) BatchUpsertExemplarMetrics(ctx context.Context, arg []BatchUpsertExemplarMetricsParams) *BatchUpsertExemplarMetricsBatchResults {
	batch := &pgx.Batch{}
	for _, a := range arg {
		vals := []interface{}{
			a.OrganizationID,
			a.ServiceIdentifierID,
			a.MetricName,
			a.MetricType,
			a.Attributes,
			a.Exemplar,
		}
		batch.Queue(batchUpsertExemplarMetrics, vals...)
	}
	br := q.db.SendBatch(ctx, batch)
	return &BatchUpsertExemplarMetricsBatchResults{br, len(arg), false}
}

func (b *BatchUpsertExemplarMetricsBatchResults) QueryRow(f func(int, bool, error)) {
	defer b.br.Close()
	for t := 0; t < b.tot; t++ {
		var is_new bool
		if b.closed {
			if f != nil {
				f(t, is_new, ErrBatchAlreadyClosed)
			}
			continue
		}
		row := b.br.QueryRow()
		err := row.Scan(&is_new)
		if f != nil {
			f(t, is_new, err)
		}
	}
}

func (b *BatchUpsertExemplarMetricsBatchResults) Close() error {
	b.closed = true
	return b.br.Close()
}

const batchUpsertExemplarTraces = `-- name: BatchUpsertExemplarTraces :batchone
INSERT INTO lrdb_exemplar_traces
( organization_id
, service_identifier_id
, fingerprint
, attributes
, exemplar
, span_name
, span_kind
)
VALUES      ( $1
            , $2
            , $3
            , $4
            , $5
            , $6
            , $7
            )
    ON CONFLICT ( organization_id
            , service_identifier_id
            , fingerprint
            )
DO UPDATE SET
           attributes        = EXCLUDED.attributes,
           exemplar          = EXCLUDED.exemplar,
           span_name         = EXCLUDED.span_name,
           span_kind         = EXCLUDED.span_kind,
           updated_at        = now()
RETURNING (created_at = updated_at) AS is_new
`

type BatchUpsertExemplarTracesBatchResults struct {
	br     pgx.BatchResults
	tot    int
	closed bool
}

type BatchUpsertExemplarTracesParams struct {
	OrganizationID      uuid.UUID      `json:"organization_id"`
	ServiceIdentifierID uuid.UUID      `json:"service_identifier_id"`
	Fingerprint         int64          `json:"fingerprint"`
	Attributes          map[string]any `json:"attributes"`
	Exemplar            map[string]any `json:"exemplar"`
	SpanName            string         `json:"span_name"`
	SpanKind            int32          `json:"span_kind"`
}

func (q *Queries) BatchUpsertExemplarTraces(ctx context.Context, arg []BatchUpsertExemplarTracesParams) *BatchUpsertExemplarTracesBatchResults {
	batch := &pgx.Batch{}
	for _, a := range arg {
		vals := []interface{}{
			a.OrganizationID,
			a.ServiceIdentifierID,
			a.Fingerprint,
			a.Attributes,
			a.Exemplar,
			a.SpanName,
			a.SpanKind,
		}
		batch.Queue(batchUpsertExemplarTraces, vals...)
	}
	br := q.db.SendBatch(ctx, batch)
	return &BatchUpsertExemplarTracesBatchResults{br, len(arg), false}
}

func (b *BatchUpsertExemplarTracesBatchResults) QueryRow(f func(int, bool, error)) {
	defer b.br.Close()
	for t := 0; t < b.tot; t++ {
		var is_new bool
		if b.closed {
			if f != nil {
				f(t, is_new, ErrBatchAlreadyClosed)
			}
			continue
		}
		row := b.br.QueryRow()
		err := row.Scan(&is_new)
		if f != nil {
			f(t, is_new, err)
		}
	}
}

func (b *BatchUpsertExemplarTracesBatchResults) Close() error {
	b.closed = true
	return b.br.Close()
}

const insertCompactedMetricSeg = `-- name: InsertCompactedMetricSeg :batchexec
INSERT INTO metric_seg (
  organization_id, dateint, frequency_ms, segment_id, instance_num,
  ts_range, record_count, file_size, ingest_dateint,
  published, rolledup, created_at, created_by, slot_id,
  fingerprints, sort_version, slot_count, compacted
)
VALUES (
  $1,
  $2,
  $3,
  $4,
  $5,
  int8range($6, $7, '[)'),  -- half-open; change to '[]' if you store inclusive end
  $8,
  $9,
  $10,
  $11,            -- typically true for new compacted output
  $12,             -- pass as needed
  now(),
  $13,
  $14,
  $15::bigint[],  -- per-row bigint[]; bind as []int64
  $16,
  $17,
  false                  -- new segments are not compacted
)
ON CONFLICT (organization_id, dateint, frequency_ms, segment_id, instance_num, slot_id, slot_count)
DO NOTHING
`

type InsertCompactedMetricSegBatchResults struct {
	br     pgx.BatchResults
	tot    int
	closed bool
}

type InsertCompactedMetricSegParams struct {
	OrganizationID uuid.UUID `json:"organization_id"`
	Dateint        int32     `json:"dateint"`
	FrequencyMs    int32     `json:"frequency_ms"`
	SegmentID      int64     `json:"segment_id"`
	InstanceNum    int16     `json:"instance_num"`
	StartTs        int64     `json:"start_ts"`
	EndTs          int64     `json:"end_ts"`
	RecordCount    int64     `json:"record_count"`
	FileSize       int64     `json:"file_size"`
	EIngestDateint int32     `json:"e_ingest_dateint"`
	Published      bool      `json:"published"`
	Rolledup       bool      `json:"rolledup"`
	CreatedBy      CreatedBy `json:"created_by"`
	SlotID         int32     `json:"slot_id"`
	Fingerprints   []int64   `json:"fingerprints"`
	SortVersion    int16     `json:"sort_version"`
	SlotCount      int32     `json:"slot_count"`
}

func (q *Queries) InsertCompactedMetricSeg(ctx context.Context, arg []InsertCompactedMetricSegParams) *InsertCompactedMetricSegBatchResults {
	batch := &pgx.Batch{}
	for _, a := range arg {
		vals := []interface{}{
			a.OrganizationID,
			a.Dateint,
			a.FrequencyMs,
			a.SegmentID,
			a.InstanceNum,
			a.StartTs,
			a.EndTs,
			a.RecordCount,
			a.FileSize,
			a.EIngestDateint,
			a.Published,
			a.Rolledup,
			a.CreatedBy,
			a.SlotID,
			a.Fingerprints,
			a.SortVersion,
			a.SlotCount,
		}
		batch.Queue(insertCompactedMetricSeg, vals...)
	}
	br := q.db.SendBatch(ctx, batch)
	return &InsertCompactedMetricSegBatchResults{br, len(arg), false}
}

func (b *InsertCompactedMetricSegBatchResults) Exec(f func(int, error)) {
	defer b.br.Close()
	for t := 0; t < b.tot; t++ {
		if b.closed {
			if f != nil {
				f(t, ErrBatchAlreadyClosed)
			}
			continue
		}
		_, err := b.br.Exec()
		if f != nil {
			f(t, err)
		}
	}
}

func (b *InsertCompactedMetricSegBatchResults) Close() error {
	b.closed = true
	return b.br.Close()
}

const kafkaJournalBatchUpsert = `-- name: KafkaJournalBatchUpsert :batchexec
INSERT INTO kafka_offset_journal (consumer_group, topic, partition, last_processed_offset, updated_at)
VALUES ($1, $2, $3, $4, NOW())
ON CONFLICT (consumer_group, topic, partition)
DO UPDATE SET 
    last_processed_offset = EXCLUDED.last_processed_offset,
    updated_at = NOW()
WHERE kafka_offset_journal.last_processed_offset < EXCLUDED.last_processed_offset
`

type KafkaJournalBatchUpsertBatchResults struct {
	br     pgx.BatchResults
	tot    int
	closed bool
}

type KafkaJournalBatchUpsertParams struct {
	ConsumerGroup       string `json:"consumer_group"`
	Topic               string `json:"topic"`
	Partition           int32  `json:"partition"`
	LastProcessedOffset int64  `json:"last_processed_offset"`
}

// Insert or update multiple Kafka journal entries in a single batch operation
// Only updates if the new offset is greater than the existing one
func (q *Queries) KafkaJournalBatchUpsert(ctx context.Context, arg []KafkaJournalBatchUpsertParams) *KafkaJournalBatchUpsertBatchResults {
	batch := &pgx.Batch{}
	for _, a := range arg {
		vals := []interface{}{
			a.ConsumerGroup,
			a.Topic,
			a.Partition,
			a.LastProcessedOffset,
		}
		batch.Queue(kafkaJournalBatchUpsert, vals...)
	}
	br := q.db.SendBatch(ctx, batch)
	return &KafkaJournalBatchUpsertBatchResults{br, len(arg), false}
}

func (b *KafkaJournalBatchUpsertBatchResults) Exec(f func(int, error)) {
	defer b.br.Close()
	for t := 0; t < b.tot; t++ {
		if b.closed {
			if f != nil {
				f(t, ErrBatchAlreadyClosed)
			}
			continue
		}
		_, err := b.br.Exec()
		if f != nil {
			f(t, err)
		}
	}
}

func (b *KafkaJournalBatchUpsertBatchResults) Close() error {
	b.closed = true
	return b.br.Close()
}
