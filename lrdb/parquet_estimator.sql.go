// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.29.0
// source: parquet_estimator.sql

package lrdb

import (
	"context"

	"github.com/google/uuid"
)

const logSegEstimator = `-- name: LogSegEstimator :many
WITH params AS (
  SELECT
    (EXTRACT(EPOCH FROM now() - INTERVAL '6 hour') * 1000)::bigint AS low_ms,
    (EXTRACT(EPOCH FROM now())              * 1000)::bigint AS high_ms
)
SELECT
  organization_id,
  instance_num,
  (sum(file_size)::float8 / sum(record_count))::float8 AS avg_bpr
FROM log_seg
CROSS JOIN params
WHERE
    record_count > 100
  AND dateint IN (
    (to_char(now(),                     'YYYYMMDD'))::int,
    (to_char(now() - INTERVAL '6 hour', 'YYYYMMDD'))::int
  )
  AND ts_range && int8range(params.low_ms, params.high_ms, '[)')
GROUP BY
  organization_id,
  instance_num
ORDER BY
  organization_id,
  instance_num
`

type LogSegEstimatorRow struct {
	OrganizationID uuid.UUID `json:"organization_id"`
	InstanceNum    int16     `json:"instance_num"`
	AvgBpr         float64   `json:"avg_bpr"`
}

// Returns an estimate of the number of log segments, average bytes, average records,
// and average bytes per record for log segments in the last hour per organization and instance.
// This query is basically identical to the MetricSegEstimator, but for log segments.
func (q *Queries) LogSegEstimator(ctx context.Context) ([]LogSegEstimatorRow, error) {
	rows, err := q.db.Query(ctx, logSegEstimator)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []LogSegEstimatorRow
	for rows.Next() {
		var i LogSegEstimatorRow
		if err := rows.Scan(&i.OrganizationID, &i.InstanceNum, &i.AvgBpr); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const metricSegEstimator = `-- name: MetricSegEstimator :many
WITH params AS (
  SELECT
    (EXTRACT(EPOCH FROM now() - INTERVAL '6 hour') * 1000)::bigint AS low_ms,
    (EXTRACT(EPOCH FROM now())              * 1000)::bigint AS high_ms
)
SELECT
  organization_id,
  instance_num,
  (sum(file_size)::float8 / sum(record_count))::float8 AS avg_bpr
FROM metric_seg
CROSS JOIN params
WHERE
    record_count > 100
  AND dateint IN (
    (to_char(now(),                     'YYYYMMDD'))::int,
    (to_char(now() - INTERVAL '6 hour', 'YYYYMMDD'))::int
  )
  AND ts_range && int8range(params.low_ms, params.high_ms, '[)')
GROUP BY
  organization_id,
  instance_num
ORDER BY
  organization_id,
  instance_num
`

type MetricSegEstimatorRow struct {
	OrganizationID uuid.UUID `json:"organization_id"`
	InstanceNum    int16     `json:"instance_num"`
	AvgBpr         float64   `json:"avg_bpr"`
}

// Returns an estimate of the number of metric segments, average bytes, average records,
// and average bytes per record for metric segments in the last hour per organization and instance.
// This query is basically identical to the LogSegEstimator, but for metric segments.
func (q *Queries) MetricSegEstimator(ctx context.Context) ([]MetricSegEstimatorRow, error) {
	rows, err := q.db.Query(ctx, metricSegEstimator)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []MetricSegEstimatorRow
	for rows.Next() {
		var i MetricSegEstimatorRow
		if err := rows.Scan(&i.OrganizationID, &i.InstanceNum, &i.AvgBpr); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}
