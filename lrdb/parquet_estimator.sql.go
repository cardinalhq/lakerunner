// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.29.0
// source: parquet_estimator.sql

package lrdb

import (
	"context"

	"github.com/google/uuid"
)

const logSegEstimator = `-- name: LogSegEstimator :many
WITH params AS (
  SELECT 1_000_000::float8 AS target_bytes
),
bpr AS (
  SELECT
    organization_id,
    instance_num,
    (sum(file_size)::float8 / NULLIF(sum(record_count), 0))::float8 AS avg_bpr
  FROM log_seg
  WHERE
      record_count > 100
      AND dateint IN ($1, $2)
      AND ts_range && int8range($3, $4, '[)')
  GROUP BY organization_id, instance_num
)
SELECT
  b.organization_id,
  b.instance_num,
  CEIL(p.target_bytes / NULLIF(b.avg_bpr, 0))::bigint AS estimated_records
FROM bpr b
CROSS JOIN params p
`

type LogSegEstimatorParams struct {
	DateintLow  int32 `json:"dateint_low"`
	DateintHigh int32 `json:"dateint_high"`
	MsLow       int64 `json:"ms_low"`
	MsHigh      int64 `json:"ms_high"`
}

type LogSegEstimatorRow struct {
	OrganizationID   uuid.UUID `json:"organization_id"`
	InstanceNum      int16     `json:"instance_num"`
	EstimatedRecords int64     `json:"estimated_records"`
}

// Returns an estimate of the number of log segments, average bytes, average records,
// and average bytes per record for log segments in the last hour per organization and instance.
// This query is basically identical to the MetricSegEstimator, but for log segments.
func (q *Queries) LogSegEstimator(ctx context.Context, arg LogSegEstimatorParams) ([]LogSegEstimatorRow, error) {
	rows, err := q.db.Query(ctx, logSegEstimator,
		arg.DateintLow,
		arg.DateintHigh,
		arg.MsLow,
		arg.MsHigh,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []LogSegEstimatorRow
	for rows.Next() {
		var i LogSegEstimatorRow
		if err := rows.Scan(&i.OrganizationID, &i.InstanceNum, &i.EstimatedRecords); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const metricSegEstimator = `-- name: MetricSegEstimator :many
WITH params AS (
  SELECT 1_000_000::float8 AS target_bytes
),
bpr AS (
  SELECT
    organization_id,
    instance_num,
    frequency_ms,
    (sum(file_size)::float8 / NULLIF(sum(record_count), 0))::float8 AS avg_bpr
  FROM metric_seg
  WHERE
      record_count > 100
      AND dateint IN ($1, $2)
      AND ts_range && int8range($3, $4, '[)')
  GROUP BY organization_id, instance_num, frequency_ms
)
SELECT
  b.organization_id,
  b.instance_num,
  b.frequency_ms,
  CEIL(p.target_bytes / NULLIF(b.avg_bpr, 0))::bigint AS estimated_records
FROM bpr b
CROSS JOIN params p
`

type MetricSegEstimatorParams struct {
	DateintLow  int32 `json:"dateint_low"`
	DateintHigh int32 `json:"dateint_high"`
	MsLow       int64 `json:"ms_low"`
	MsHigh      int64 `json:"ms_high"`
}

type MetricSegEstimatorRow struct {
	OrganizationID   uuid.UUID `json:"organization_id"`
	InstanceNum      int16     `json:"instance_num"`
	FrequencyMs      int32     `json:"frequency_ms"`
	EstimatedRecords int64     `json:"estimated_records"`
}

// Returns an estimate of the number of metric segments, average bytes, average records,
// and average bytes per record for metric segments in the last hour per organization, instance, and frequency.
// Uses frequency_ms to provide more accurate estimates based on collection frequency.
func (q *Queries) MetricSegEstimator(ctx context.Context, arg MetricSegEstimatorParams) ([]MetricSegEstimatorRow, error) {
	rows, err := q.db.Query(ctx, metricSegEstimator,
		arg.DateintLow,
		arg.DateintHigh,
		arg.MsLow,
		arg.MsHigh,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []MetricSegEstimatorRow
	for rows.Next() {
		var i MetricSegEstimatorRow
		if err := rows.Scan(
			&i.OrganizationID,
			&i.InstanceNum,
			&i.FrequencyMs,
			&i.EstimatedRecords,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}
