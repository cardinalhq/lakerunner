// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.29.0
// source: parquet_estimator.sql

package lrdb

import (
	"context"

	"github.com/google/uuid"
)

const logSegEstimator = `-- name: LogSegEstimator :many
WITH params AS (
  SELECT 
    1000000::float8 AS target_bytes,
    15000::float8 AS estimated_overhead_per_file  -- Based on observed 10-18K overhead
),
stats AS (
  SELECT
    organization_id,
    COUNT(*) AS file_count,
    SUM(file_size) AS total_bytes,
    SUM(record_count) AS total_records
  FROM log_seg
  WHERE
      record_count > 100
      AND dateint IN ($1, $2)
      AND ts_range && int8range($3, $4, '[)')
  GROUP BY organization_id
),
estimates AS (
  SELECT
    organization_id,
    -- Estimate bytes per record excluding overhead
    CASE 
      WHEN total_records > 0 AND file_count > 0 THEN
        GREATEST(1, (total_bytes::float8 - (file_count * p.estimated_overhead_per_file)) / total_records)
      ELSE 100  -- fallback bytes per record
    END AS bytes_per_record
  FROM stats s
  CROSS JOIN params p
)
SELECT
  e.organization_id,
  -- Calculate records needed: (target_bytes - overhead) / bytes_per_record
  GREATEST(1000, 
    CEIL((p.target_bytes - p.estimated_overhead_per_file) / NULLIF(e.bytes_per_record, 0))
  )::bigint AS estimated_records
FROM estimates e
CROSS JOIN params p
`

type LogSegEstimatorParams struct {
	DateintLow  int32 `json:"dateint_low"`
	DateintHigh int32 `json:"dateint_high"`
	MsLow       int64 `json:"ms_low"`
	MsHigh      int64 `json:"ms_high"`
}

type LogSegEstimatorRow struct {
	OrganizationID   uuid.UUID `json:"organization_id"`
	EstimatedRecords int64     `json:"estimated_records"`
}

// Returns an estimate of the number of log segments, accounting for per-file overhead.
func (q *Queries) LogSegEstimator(ctx context.Context, arg LogSegEstimatorParams) ([]LogSegEstimatorRow, error) {
	rows, err := q.db.Query(ctx, logSegEstimator,
		arg.DateintLow,
		arg.DateintHigh,
		arg.MsLow,
		arg.MsHigh,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []LogSegEstimatorRow
	for rows.Next() {
		var i LogSegEstimatorRow
		if err := rows.Scan(&i.OrganizationID, &i.EstimatedRecords); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const metricSegEstimator = `-- name: MetricSegEstimator :many
WITH params AS (
  SELECT 
    1000000::float8 AS target_bytes,
    15000::float8 AS estimated_overhead_per_file  -- Based on observed 10-18K overhead
),
stats AS (
  SELECT
    organization_id,
    frequency_ms,
    COUNT(*) AS file_count,
    SUM(file_size) AS total_bytes,
    SUM(record_count) AS total_records
  FROM metric_seg
  WHERE
      record_count > 100
      AND dateint IN ($1, $2)
      AND ts_range && int8range($3, $4, '[)')
  GROUP BY organization_id, frequency_ms
),
estimates AS (
  SELECT
    organization_id,
    frequency_ms,
    -- Estimate bytes per record excluding overhead
    CASE 
      WHEN total_records > 0 AND file_count > 0 THEN
        GREATEST(1, (total_bytes::float8 - (file_count * p.estimated_overhead_per_file)) / total_records)
      ELSE 100  -- fallback bytes per record
    END AS bytes_per_record
  FROM stats s
  CROSS JOIN params p
)
SELECT
  e.organization_id,
  e.frequency_ms,
  -- Calculate records needed: (target_bytes - overhead) / bytes_per_record
  GREATEST(1000, 
    CEIL((p.target_bytes - p.estimated_overhead_per_file) / NULLIF(e.bytes_per_record, 0))
  )::bigint AS estimated_records
FROM estimates e
CROSS JOIN params p
`

type MetricSegEstimatorParams struct {
	DateintLow  int32 `json:"dateint_low"`
	DateintHigh int32 `json:"dateint_high"`
	MsLow       int64 `json:"ms_low"`
	MsHigh      int64 `json:"ms_high"`
}

type MetricSegEstimatorRow struct {
	OrganizationID   uuid.UUID `json:"organization_id"`
	FrequencyMs      int32     `json:"frequency_ms"`
	EstimatedRecords int64     `json:"estimated_records"`
}

// Returns an estimate of the number of metric segments, accounting for per-file overhead.
// Uses frequency_ms to provide more accurate estimates based on collection frequency.
func (q *Queries) MetricSegEstimator(ctx context.Context, arg MetricSegEstimatorParams) ([]MetricSegEstimatorRow, error) {
	rows, err := q.db.Query(ctx, metricSegEstimator,
		arg.DateintLow,
		arg.DateintHigh,
		arg.MsLow,
		arg.MsHigh,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []MetricSegEstimatorRow
	for rows.Next() {
		var i MetricSegEstimatorRow
		if err := rows.Scan(&i.OrganizationID, &i.FrequencyMs, &i.EstimatedRecords); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const traceSegEstimator = `-- name: TraceSegEstimator :many
WITH params AS (
  SELECT 
    1000000::float8 AS target_bytes,
    15000::float8 AS estimated_overhead_per_file  -- Based on observed 10-18K overhead
),
stats AS (
  SELECT
    organization_id,
    COUNT(*) AS file_count,
    SUM(file_size) AS total_bytes,
    SUM(record_count) AS total_records
  FROM trace_seg
  WHERE
      record_count > 100
      AND dateint IN ($1, $2)
      AND ts_range && int8range($3, $4, '[)')
  GROUP BY organization_id
),
estimates AS (
  SELECT
    organization_id,
    -- Estimate bytes per record excluding overhead
    CASE 
      WHEN total_records > 0 AND file_count > 0 THEN
        GREATEST(1, (total_bytes::float8 - (file_count * p.estimated_overhead_per_file)) / total_records)
      ELSE 100  -- fallback bytes per record
    END AS bytes_per_record
  FROM stats s
  CROSS JOIN params p
)
SELECT
  e.organization_id,
  -- Calculate records needed: (target_bytes - overhead) / bytes_per_record
  GREATEST(1000, 
    CEIL((p.target_bytes - p.estimated_overhead_per_file) / NULLIF(e.bytes_per_record, 0))
  )::bigint AS estimated_records
FROM estimates e
CROSS JOIN params p
`

type TraceSegEstimatorParams struct {
	DateintLow  int32 `json:"dateint_low"`
	DateintHigh int32 `json:"dateint_high"`
	MsLow       int64 `json:"ms_low"`
	MsHigh      int64 `json:"ms_high"`
}

type TraceSegEstimatorRow struct {
	OrganizationID   uuid.UUID `json:"organization_id"`
	EstimatedRecords int64     `json:"estimated_records"`
}

// Returns an estimate of the number of trace segments, accounting for per-file overhead.
func (q *Queries) TraceSegEstimator(ctx context.Context, arg TraceSegEstimatorParams) ([]TraceSegEstimatorRow, error) {
	rows, err := q.db.Query(ctx, traceSegEstimator,
		arg.DateintLow,
		arg.DateintHigh,
		arg.MsLow,
		arg.MsHigh,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []TraceSegEstimatorRow
	for rows.Next() {
		var i TraceSegEstimatorRow
		if err := rows.Scan(&i.OrganizationID, &i.EstimatedRecords); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}
