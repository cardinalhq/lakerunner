// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.29.0
// source: mcq_claim_batch.sql

package lrdb

import (
	"context"
	"time"

	"github.com/google/uuid"
	"github.com/jackc/pgx/v5/pgtype"
)

const claimMetricCompactionWorkBatch = `-- name: ClaimMetricCompactionWorkBatch :many
WITH
params AS (
  SELECT
    $1::uuid     AS organization_id,
    $2::smallint    AS instance_num,
    $3::bigint         AS worker_id,
    COALESCE($4::timestamptz, now()) AS now_ts,
    $5::bigint       AS max_records,       -- hard cap on number of records
    $6::bigint       AS min_records,       -- minimum records for fresh items
    $7::integer  AS max_age_seconds,   -- age threshold for taking whatever is available
    $8::integer      AS batch_count        -- row cap (no more than this many items)
),
scope AS (
  SELECT q.id, q.queue_ts, q.priority, q.organization_id, q.dateint, q.frequency_ms, q.segment_id, q.instance_num, q.ts_range, q.record_count, q.tries, q.claimed_by, q.claimed_at
  FROM metric_compaction_queue q
  JOIN params p ON TRUE
  WHERE q.claimed_at IS NULL
    AND q.organization_id = p.organization_id
    AND q.instance_num    = p.instance_num
  ORDER BY q.priority DESC, q.queue_ts ASC
  FOR UPDATE SKIP LOCKED
),
oldest AS (
  SELECT id, queue_ts, priority, organization_id, dateint, frequency_ms, segment_id, instance_num, ts_range, record_count, tries, claimed_by, claimed_at FROM scope LIMIT 1
),
flags AS (
  SELECT
    (o.record_count >  p.max_records)                               AS is_oversized,
    ((p.now_ts - o.queue_ts) > make_interval(secs => p.max_age_seconds)) AS is_old,
    p.organization_id, p.instance_num, p.worker_id, p.now_ts, p.max_records, p.min_records, p.max_age_seconds, p.batch_count
  FROM params p
  JOIN oldest o ON TRUE
),
pack AS (
  SELECT
    s.id, s.queue_ts, s.priority, s.organization_id, s.dateint, s.frequency_ms, s.segment_id, s.instance_num, s.ts_range, s.record_count, s.tries, s.claimed_by, s.claimed_at,
    SUM(s.record_count) OVER (ORDER BY s.priority DESC, s.queue_ts ASC
                           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cum_records,
    ROW_NUMBER() OVER (ORDER BY s.priority DESC, s.queue_ts ASC)              AS rn
  FROM scope s
),
prelim AS (
  SELECT p.id, p.queue_ts, p.priority, p.organization_id, p.dateint, p.frequency_ms, p.segment_id, p.instance_num, p.ts_range, p.record_count, p.tries, p.claimed_by, p.claimed_at, p.cum_records, p.rn
  FROM pack p
  JOIN flags f ON TRUE
  WHERE p.cum_records <= f.max_records
    AND p.rn         <= f.batch_count
),
prelim_stats AS (
  SELECT
    COUNT(*) AS n_rows,
    COALESCE(SUM(record_count), 0) AS total_records
  FROM prelim
),
chosen AS (
  -- 1) Oversized oldest -> claim just that one  
  (
    SELECT o.id, o.queue_ts, o.priority, o.organization_id, o.dateint, 
           o.frequency_ms, o.segment_id, o.instance_num, o.ts_range, o.record_count, 
           o.tries, o.claimed_by, o.claimed_at
    FROM oldest o
    JOIN flags f ON TRUE
    WHERE f.is_oversized
  )
  UNION ALL
  -- 2) Too old -> take whatever fits under caps (ignore min records)
  (
    SELECT p.id, p.queue_ts, p.priority, p.organization_id, p.dateint, 
           p.frequency_ms, p.segment_id, p.instance_num, p.ts_range, p.record_count, 
           p.tries, p.claimed_by, p.claimed_at
    FROM prelim p
    JOIN flags f ON TRUE
    WHERE NOT f.is_oversized AND f.is_old
  )
  UNION ALL
  -- 3) Fresh -> only if record minimum met (and caps already enforced by prelim)
  (
    SELECT p.id, p.queue_ts, p.priority, p.organization_id, p.dateint, 
           p.frequency_ms, p.segment_id, p.instance_num, p.ts_range, p.record_count, 
           p.tries, p.claimed_by, p.claimed_at
    FROM prelim p
    JOIN flags f ON TRUE
    JOIN prelim_stats ps ON TRUE
    WHERE NOT f.is_oversized
      AND NOT f.is_old
      AND ps.total_records >= f.min_records
  )
),
upd AS (
  UPDATE metric_compaction_queue q
  SET claimed_by = (SELECT worker_id FROM params),
      claimed_at = (SELECT now_ts FROM params)
  FROM chosen c
  WHERE q.id = c.id
  RETURNING q.id, q.queue_ts, q.priority, q.organization_id, q.dateint, q.frequency_ms, q.segment_id, q.instance_num, q.ts_range, q.record_count, q.tries, q.claimed_by, q.claimed_at
)
SELECT id, queue_ts, priority, organization_id, dateint, frequency_ms, segment_id, instance_num, ts_range, record_count, tries, claimed_by, claimed_at FROM upd
ORDER BY priority DESC, queue_ts ASC
`

type ClaimMetricCompactionWorkBatchParams struct {
	OrganizationID uuid.UUID  `json:"organization_id"`
	InstanceNum    int16      `json:"instance_num"`
	WorkerID       int64      `json:"worker_id"`
	NowTs          *time.Time `json:"now_ts"`
	MaxRecords     int64      `json:"max_records"`
	MinRecords     int64      `json:"min_records"`
	MaxAgeSeconds  int32      `json:"max_age_seconds"`
	BatchCount     int32      `json:"batch_count"`
}

type ClaimMetricCompactionWorkBatchRow struct {
	ID             int64                            `json:"id"`
	QueueTs        time.Time                        `json:"queue_ts"`
	Priority       int32                            `json:"priority"`
	OrganizationID uuid.UUID                        `json:"organization_id"`
	Dateint        int32                            `json:"dateint"`
	FrequencyMs    int64                            `json:"frequency_ms"`
	SegmentID      uuid.UUID                        `json:"segment_id"`
	InstanceNum    int16                            `json:"instance_num"`
	TsRange        pgtype.Range[pgtype.Timestamptz] `json:"ts_range"`
	RecordCount    int64                            `json:"record_count"`
	Tries          int32                            `json:"tries"`
	ClaimedBy      int64                            `json:"claimed_by"`
	ClaimedAt      *time.Time                       `json:"claimed_at"`
}

// Greedy pack up to record cap and row cap
func (q *Queries) ClaimMetricCompactionWorkBatch(ctx context.Context, arg ClaimMetricCompactionWorkBatchParams) ([]ClaimMetricCompactionWorkBatchRow, error) {
	rows, err := q.db.Query(ctx, claimMetricCompactionWorkBatch,
		arg.OrganizationID,
		arg.InstanceNum,
		arg.WorkerID,
		arg.NowTs,
		arg.MaxRecords,
		arg.MinRecords,
		arg.MaxAgeSeconds,
		arg.BatchCount,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ClaimMetricCompactionWorkBatchRow
	for rows.Next() {
		var i ClaimMetricCompactionWorkBatchRow
		if err := rows.Scan(
			&i.ID,
			&i.QueueTs,
			&i.Priority,
			&i.OrganizationID,
			&i.Dateint,
			&i.FrequencyMs,
			&i.SegmentID,
			&i.InstanceNum,
			&i.TsRange,
			&i.RecordCount,
			&i.Tries,
			&i.ClaimedBy,
			&i.ClaimedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}
