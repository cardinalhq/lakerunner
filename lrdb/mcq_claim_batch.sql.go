// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.29.0
// source: mcq_claim_batch.sql

package lrdb

import (
	"context"
	"time"

	"github.com/google/uuid"
)

const claimMetricCompactionWork = `-- name: ClaimMetricCompactionWork :many
WITH
params AS (
  SELECT
    $1::bigint                               AS worker_id,
    COALESCE($2::timestamptz, now())  AS now_ts,
    $3::bigint                  AS default_target_records,
    $4::integer                        AS max_age_seconds,
    $5::integer                            AS batch_count
),

big_single AS (
  SELECT q.id
  FROM metric_compaction_queue q
  JOIN params p ON TRUE
  LEFT JOIN metric_pack_estimate e_org
         ON e_org.organization_id = q.organization_id
        AND e_org.frequency_ms    = q.frequency_ms
  LEFT JOIN metric_pack_estimate e_glob
         ON e_glob.organization_id = '00000000-0000-0000-0000-000000000000'::uuid
        AND e_glob.frequency_ms    = q.frequency_ms
  CROSS JOIN LATERAL (
    SELECT COALESCE(e_org.target_records, e_glob.target_records, p.default_target_records)::bigint AS target_records
  ) trg
  WHERE q.claimed_at IS NULL
    AND q.record_count >= trg.target_records
  ORDER BY q.priority DESC, q.queue_ts ASC, q.id ASC
  LIMIT 1
),

seeds_per_group AS (
  SELECT DISTINCT ON (organization_id, dateint, frequency_ms, instance_num)
         id AS seed_id, organization_id, dateint, frequency_ms, instance_num,
         priority, queue_ts, record_count
  FROM metric_compaction_queue
  WHERE claimed_at IS NULL
  ORDER BY organization_id, dateint, frequency_ms, instance_num, priority DESC, queue_ts ASC, id ASC
),

ordered_groups AS (
  SELECT s.seed_id, s.organization_id, s.dateint, s.frequency_ms, s.instance_num, s.priority, s.queue_ts, s.record_count,
         ROW_NUMBER() OVER (ORDER BY s.priority DESC, s.queue_ts ASC, s.seed_id ASC) AS seed_rank
  FROM seeds_per_group s
),

group_analysis AS (
  SELECT
    og.organization_id, og.dateint, og.frequency_ms, og.instance_num,
    og.priority, og.queue_ts, og.seed_rank,
    -- Age and target calculation
    ((p.now_ts - og.queue_ts) > make_interval(secs => p.max_age_seconds)) AS is_old,
    COALESCE(e_org.target_records, e_glob.target_records, p.default_target_records)::bigint AS target_records,
    -- Calculate total available records for this group
    COALESCE(SUM(q.record_count), 0) AS total_available_records,
    COUNT(*) AS total_items,
    e_org.target_records AS org_estimate,
    e_glob.target_records AS global_estimate,
    p.default_target_records AS default_estimate,
    CASE 
      WHEN e_org.target_records IS NOT NULL THEN 'organization'
      WHEN e_glob.target_records IS NOT NULL THEN 'global'
      ELSE 'default'
    END AS estimate_source,
    p.batch_count,
    p.now_ts
  FROM ordered_groups og
  CROSS JOIN params p
  LEFT JOIN metric_pack_estimate e_org
         ON e_org.organization_id = og.organization_id
        AND e_org.frequency_ms    = og.frequency_ms
  LEFT JOIN metric_pack_estimate e_glob
         ON e_glob.organization_id = '00000000-0000-0000-0000-000000000000'::uuid
        AND e_glob.frequency_ms    = og.frequency_ms
  -- Join with all unclaimed items in this group
  LEFT JOIN metric_compaction_queue q
         ON q.claimed_at IS NULL
        AND q.organization_id = og.organization_id
        AND q.dateint = og.dateint
        AND q.frequency_ms = og.frequency_ms
        AND q.instance_num = og.instance_num
  GROUP BY og.organization_id, og.dateint, og.frequency_ms, og.instance_num,
           og.priority, og.queue_ts, og.seed_rank, p.now_ts, p.max_age_seconds,
           e_org.target_records, e_glob.target_records, p.default_target_records,
           p.batch_count
),

eligible_groups AS (
  SELECT 
    ga.organization_id, ga.dateint, ga.frequency_ms, ga.instance_num, ga.priority, ga.queue_ts, ga.seed_rank, ga.is_old, ga.target_records, ga.total_available_records, ga.total_items, ga.org_estimate, ga.global_estimate, ga.default_estimate, ga.estimate_source, ga.batch_count, ga.now_ts,
    CASE 
      WHEN ga.is_old THEN true
      WHEN ga.total_available_records >= ga.target_records THEN true
      ELSE false
    END AS is_eligible,
    CASE 
      WHEN ga.is_old THEN 'old'
      WHEN ga.total_available_records >= ga.target_records THEN 'full_batch'
      ELSE 'insufficient'
    END AS eligibility_reason
  FROM group_analysis ga
  WHERE ga.total_available_records > 0
),

winner_group AS (
  SELECT eg.organization_id, eg.dateint, eg.frequency_ms, eg.instance_num, eg.priority, eg.queue_ts, eg.seed_rank, eg.is_old, eg.target_records, eg.total_available_records, eg.total_items, eg.org_estimate, eg.global_estimate, eg.default_estimate, eg.estimate_source, eg.batch_count, eg.now_ts, eg.is_eligible, eg.eligibility_reason
  FROM eligible_groups eg
  WHERE eg.is_eligible = true
  ORDER BY eg.seed_rank ASC
  LIMIT 1
),

group_items AS (
  SELECT
    q.id, q.organization_id, q.dateint, q.frequency_ms, q.instance_num,
    q.priority, q.queue_ts, q.record_count,
    wg.seed_rank, wg.is_old, wg.target_records, wg.batch_count,
    wg.org_estimate, wg.global_estimate, wg.default_estimate, wg.estimate_source,
    wg.eligibility_reason
  FROM metric_compaction_queue q
  JOIN winner_group wg
    ON q.claimed_at   IS NULL
   AND q.organization_id = wg.organization_id
   AND q.dateint         = wg.dateint
   AND q.frequency_ms    = wg.frequency_ms
   AND q.instance_num    = wg.instance_num
  ORDER BY q.priority DESC, q.queue_ts ASC, q.id ASC
),

pack AS (
  SELECT
    gi.id, gi.organization_id, gi.dateint, gi.frequency_ms, gi.instance_num, gi.priority, gi.queue_ts, gi.record_count, gi.seed_rank, gi.is_old, gi.target_records, gi.batch_count, gi.org_estimate, gi.global_estimate, gi.default_estimate, gi.estimate_source, gi.eligibility_reason,
    SUM(gi.record_count) OVER (
      ORDER BY gi.priority DESC, gi.queue_ts ASC, gi.id ASC
      ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
    ) AS cum_records,
    ROW_NUMBER() OVER (
      ORDER BY gi.priority DESC, gi.queue_ts ASC, gi.id ASC
    ) AS rn
  FROM group_items gi
),

pack_with_limits AS (
  SELECT p.id, p.organization_id, p.dateint, p.frequency_ms, p.instance_num, p.priority, p.queue_ts, p.record_count, p.seed_rank, p.is_old, p.target_records, p.batch_count, p.org_estimate, p.global_estimate, p.default_estimate, p.estimate_source, p.eligibility_reason, p.cum_records, p.rn
  FROM pack p
  JOIN winner_group wg ON TRUE
  WHERE 
    -- For old batches: take up to batch_count items regardless of total records
    (wg.eligibility_reason = 'old' AND p.rn <= wg.batch_count)
    OR
    -- For full batches: greedily pack up to 120% of target to make efficient batches
    (wg.eligibility_reason = 'full_batch' 
     AND p.cum_records <= (wg.target_records * 1.2)
     AND p.rn <= wg.batch_count)
),

final_selection AS (
  SELECT pwl.id, pwl.organization_id, pwl.dateint, pwl.frequency_ms, pwl.instance_num, pwl.priority, pwl.queue_ts, pwl.record_count, pwl.seed_rank, pwl.is_old, pwl.target_records, pwl.batch_count, pwl.org_estimate, pwl.global_estimate, pwl.default_estimate, pwl.estimate_source, pwl.eligibility_reason, pwl.cum_records, pwl.rn
  FROM pack_with_limits pwl
  JOIN winner_group wg ON TRUE
  WHERE 
    -- For old batches: keep all items (no overshoot protection)
    wg.eligibility_reason = 'old'
    OR
    -- For full batches: drop the last item if it causes >20% overshoot and we have >=2 items
    (wg.eligibility_reason = 'full_batch' AND (
      -- Keep if we're not over 20%
      pwl.cum_records <= (wg.target_records * 1.2)
      OR
      -- Keep if we only have 1 item (don't drop everything)
      (SELECT COUNT(*) FROM pack_with_limits) <= 1
      OR
      -- Keep if we're not the last item
      pwl.rn < (SELECT MAX(rn) FROM pack_with_limits)
    ))
),

chosen AS (
  SELECT id FROM big_single
  UNION ALL
  SELECT id FROM final_selection
  WHERE NOT EXISTS (SELECT 1 FROM big_single)
),

upd AS (
  UPDATE metric_compaction_queue q
  SET claimed_by = (SELECT worker_id FROM params),
      claimed_at = (SELECT now_ts FROM params),
      heartbeated_at = (SELECT now_ts FROM params)
  FROM chosen c
  WHERE q.id = c.id
    AND q.claimed_at IS NULL
  RETURNING q.id, q.queue_ts, q.priority, q.organization_id, q.dateint, q.frequency_ms, q.segment_id, q.instance_num, q.record_count, q.tries, q.claimed_by, q.claimed_at, q.heartbeated_at, q.eligible_at
)
SELECT 
  upd.id, upd.queue_ts, upd.priority, upd.organization_id, upd.dateint, upd.frequency_ms, upd.segment_id, upd.instance_num, upd.record_count, upd.tries, upd.claimed_by, upd.claimed_at, upd.heartbeated_at, upd.eligible_at,
  COALESCE(fs.target_records, est.target_records, p.default_target_records) AS used_target_records,
  COALESCE(fs.org_estimate, est.org_estimate, 0) AS org_estimate,
  COALESCE(fs.global_estimate, est.global_estimate, 0) AS global_estimate, 
  COALESCE(fs.default_estimate, est.default_estimate, p.default_target_records) AS default_estimate,
  COALESCE(fs.estimate_source, est.estimate_source, 'default') AS estimate_source,
  COALESCE(fs.eligibility_reason, 'big_single') AS batch_reason
FROM upd
CROSS JOIN params p
LEFT JOIN final_selection fs ON upd.id = fs.id
LEFT JOIN LATERAL (
  SELECT 
    COALESCE(e_org.target_records, e_glob.target_records, p2.default_target_records)::bigint AS target_records,
    e_org.target_records AS org_estimate,
    e_glob.target_records AS global_estimate,
    p2.default_target_records AS default_estimate,
    CASE 
      WHEN e_org.target_records IS NOT NULL THEN 'organization'
      WHEN e_glob.target_records IS NOT NULL THEN 'global'
      ELSE 'default'
    END AS estimate_source
  FROM params p2
  LEFT JOIN metric_pack_estimate e_org
         ON e_org.organization_id = upd.organization_id AND e_org.frequency_ms = upd.frequency_ms
  LEFT JOIN metric_pack_estimate e_glob
         ON e_glob.organization_id = '00000000-0000-0000-0000-000000000000'::uuid
        AND e_glob.frequency_ms = upd.frequency_ms
) est ON fs.id IS NULL
ORDER BY upd.priority DESC, upd.queue_ts ASC, upd.id ASC
`

type ClaimMetricCompactionWorkParams struct {
	WorkerID             int64      `json:"worker_id"`
	NowTs                *time.Time `json:"now_ts"`
	DefaultTargetRecords int64      `json:"default_target_records"`
	MaxAgeSeconds        int32      `json:"max_age_seconds"`
	BatchCount           int32      `json:"batch_count"`
}

type ClaimMetricCompactionWorkRow struct {
	ID                int64      `json:"id"`
	QueueTs           time.Time  `json:"queue_ts"`
	Priority          int32      `json:"priority"`
	OrganizationID    uuid.UUID  `json:"organization_id"`
	Dateint           int32      `json:"dateint"`
	FrequencyMs       int32      `json:"frequency_ms"`
	SegmentID         int64      `json:"segment_id"`
	InstanceNum       int16      `json:"instance_num"`
	RecordCount       int64      `json:"record_count"`
	Tries             int32      `json:"tries"`
	ClaimedBy         int64      `json:"claimed_by"`
	ClaimedAt         *time.Time `json:"claimed_at"`
	HeartbeatedAt     *time.Time `json:"heartbeated_at"`
	EligibleAt        time.Time  `json:"eligible_at"`
	UsedTargetRecords int64      `json:"used_target_records"`
	OrgEstimate       int64      `json:"org_estimate"`
	GlobalEstimate    int64      `json:"global_estimate"`
	DefaultEstimate   int64      `json:"default_estimate"`
	EstimateSource    string     `json:"estimate_source"`
	BatchReason       string     `json:"batch_reason"`
}

// 1) Big single-row safety net
// 2) One seed per group
// 3) Order groups globally by seed recency/priority
// 4) Calculate group stats and eligibility criteria
// 5) Determine group eligibility: old OR can make a full batch (>= target records)
// 6) Pick the first eligible group (ordered by seed_rank)
// 7) For the winner group, get items in priority order and pack greedily
// 8) Pack items greedily within limits
// 9) Apply limits based on eligibility reason with overshoot protection
// 10) Check for >20% overshoot and drop last item if needed (but keep at least 1 item)
// 11) Final chosen IDs
// 12) Atomic optimistic claim
// For big_single items, get estimates separately
func (q *Queries) ClaimMetricCompactionWork(ctx context.Context, arg ClaimMetricCompactionWorkParams) ([]ClaimMetricCompactionWorkRow, error) {
	rows, err := q.db.Query(ctx, claimMetricCompactionWork,
		arg.WorkerID,
		arg.NowTs,
		arg.DefaultTargetRecords,
		arg.MaxAgeSeconds,
		arg.BatchCount,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []ClaimMetricCompactionWorkRow
	for rows.Next() {
		var i ClaimMetricCompactionWorkRow
		if err := rows.Scan(
			&i.ID,
			&i.QueueTs,
			&i.Priority,
			&i.OrganizationID,
			&i.Dateint,
			&i.FrequencyMs,
			&i.SegmentID,
			&i.InstanceNum,
			&i.RecordCount,
			&i.Tries,
			&i.ClaimedBy,
			&i.ClaimedAt,
			&i.HeartbeatedAt,
			&i.EligibleAt,
			&i.UsedTargetRecords,
			&i.OrgEstimate,
			&i.GlobalEstimate,
			&i.DefaultEstimate,
			&i.EstimateSource,
			&i.BatchReason,
		); err != nil {
			return nil, err
		}
		items = append(items, i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}
