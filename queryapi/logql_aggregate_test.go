// Copyright (C) 2025 CardinalHQ, Inc
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as
// published by the Free Software Foundation, version 3.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program. If not, see <http://www.gnu.org/licenses/>.

package queryapi

import (
	"database/sql"
	"fmt"
	"github.com/cardinalhq/lakerunner/logql"
	"github.com/cardinalhq/lakerunner/promql"
	_ "github.com/marcboeker/go-duckdb/v2"

	"log/slog"
	"strconv"
	"strings"
	"testing"
	"time"
)

// -------- AGGREGATE tests ------------------------------------------------

// For worker SQL generated by BaseExpr.ToWorkerSQL(...)
// We need to replace {table}, {start}, {end}.
func replaceWorkerPlaceholders(sql string, start, end int64) string {
	sql = strings.ReplaceAll(sql, "{table}", "logs")
	sql = strings.ReplaceAll(sql, "{start}", fmt.Sprintf("%d", start))
	sql = strings.ReplaceAll(sql, "{end}", fmt.Sprintf("%d", end))
	return sql
}

// Map the LogLeafs back into BaseExpr leaves using the __leaf matcher.
func attachLeavesToPlan(t *testing.T, rr promql.RewriteResult, plan promql.QueryPlan) {
	t.Helper()
	for i := range plan.Leaves {
		be := &plan.Leaves[i] // mutate slice element
		found := false
		var kept []promql.LabelMatch
		for _, m := range be.Matchers {
			if m.Label == promql.LeafMatcher {
				leaf, ok := rr.Leaves[m.Value]
				if !ok {
					t.Fatalf("unknown __leaf id: %s", m.Value)
				}
				be.LogLeaf = &leaf
				found = true
				continue // drop __leaf from matchers
			}
			kept = append(kept, m)
		}
		if !found {
			t.Fatalf("base expr missing __leaf matcher")
		}
		if be.LogLeaf.ID == "" {
			t.Fatalf("attached LogLeaf has empty ID")
		}
		be.Matchers = kept // strip __leaf matcher
	}
}

// End-to-end: LogQL -> CompileLog (leaves) -> RewriteToPromQL -> parse PromQL -> Compile
// -> attach leaves -> return the compiled plan (typically 1 leaf).
func compileMetricPlanFromLogQL(t *testing.T, q string) (promql.QueryPlan, promql.RewriteResult) {
	t.Helper()

	ast, err := logql.FromLogQL(q)
	if err != nil {
		t.Fatalf("FromLogQL error: %v", err)
	}
	lplan, err := logql.CompileLog(ast)
	if err != nil {
		t.Fatalf("CompileLog error: %v", err)
	}

	rr, err := promql.RewriteToPromQL(lplan.Root)
	if err != nil {
		t.Fatalf("RewriteToPromQL error: %v", err)
	}
	promExpr, err := promql.FromPromQL(rr.PromQL)
	if err != nil {
		t.Fatalf("parse rewritten PromQL: %v (sql=%s)", err, rr.PromQL)
	}
	plan, err := promql.Compile(promExpr)
	if err != nil {
		t.Fatalf("compile rewritten PromQL: %v", err)
	}
	attachLeavesToPlan(t, rr, plan)
	return plan, rr
}

// pull typed values (like in your existing tests)
func s(v any) string {
	switch x := v.(type) {
	case nil:
		return ""
	case string:
		return x
	case []byte:
		return string(x)
	default:
		return fmt.Sprintf("%v", x)
	}
}

func i64(v any) int64 {
	switch x := v.(type) {
	case int64:
		return x
	case int32:
		return int64(x)
	case int16:
		return int64(x)
	case int8:
		return int64(x)
	case int:
		return int64(x)
	case uint64:
		return int64(x)
	case uint32:
		return int64(x)
	case uint16:
		return int64(x)
	case uint8:
		return int64(x)
	case float64:
		return int64(x)
	case float32:
		return int64(x)
	case []byte:
		// try parsing if driver returned numeric as bytes
		if n, err := strconv.ParseInt(string(x), 10, 64); err == nil {
			return n
		}
		return 0
	default:
		// last resort: string then parse
		if n, err := strconv.ParseInt(fmt.Sprintf("%v", x), 10, 64); err == nil {
			return n
		}
		return 0
	}
}

func openDuckDB(t *testing.T) *sql.DB {
	db, err := sql.Open("duckdb", "")
	if err != nil {
		slog.Error("Error opening duckdb for local parquet", "error", err.Error())
		t.Fatalf("open duckdb: %v", err)
	}
	t.Cleanup(func() { _ = db.Close() })
	return db
}

func mustExec(t *testing.T, db *sql.DB, q string, args ...any) {
	t.Helper()
	if _, err := db.Exec(q, args...); err != nil {
		t.Fatalf("exec failed: %v\nsql:\n%s", err, q)
	}
}

type rowmap map[string]any

func queryAll(t *testing.T, db *sql.DB, q string) []rowmap {
	t.Helper()
	rows, err := db.Query(q)
	if err != nil {
		t.Fatalf("query failed: %v\nsql:\n%s", err, q)
	}
	defer rows.Close()

	cols, err := rows.Columns()
	if err != nil {
		t.Fatalf("columns failed: %v", err)
	}

	var out []rowmap
	for rows.Next() {
		raw := make([]any, len(cols))
		ptrs := make([]any, len(cols))
		for i := range raw {
			ptrs[i] = &raw[i]
		}
		if err := rows.Scan(ptrs...); err != nil {
			t.Fatalf("scan failed: %v", err)
		}
		m := make(rowmap, len(cols))
		for i, c := range cols {
			m[c] = raw[i]
		}
		out = append(out, m)
	}
	if err := rows.Err(); err != nil {
		t.Fatalf("rows err: %v", err)
	}
	return out
}

// --- Tests -----------------------------------------------------------------

// 1) count_over_time grouped by json field (level)
func TestRewrite_CountOverTime_ByLevel_JSON(t *testing.T) {
	db := openDuckDB(t)
	// Table must include exemplar defaults the LogQL pipeline always projects.
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Data: 3 events in two 1m buckets (0-60s, 60-120s)
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"level":"INFO","user":"alice","msg":"a"}'),
	(30*1000,  'svc', '{"level":"ERROR","user":"bob","msg":"b"}'),
	(70*1000,  'svc', '{"level":"ERROR","user":"carol","msg":"c"}');
	`)

	// LogQL: count_over_time over 1m, sum by (level)
	q := `sum by (level) (count_over_time({job="svc"} | json [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	// Worker SQL for step=1m
	step := time.Minute
	sql := be.ToWorkerSQL(step)

	// Concretize placeholders for [0, 120s)
	sql = replaceWorkerPlaceholders(sql, 0, 120*1000)

	rows := queryAll(t, db, sql)

	type agg struct {
		bucket int64
		level  string
		sum    int64
	}
	var got []agg
	for _, r := range rows {
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			level:  s(r["level"]),
			sum:    i64(r["sum"]),
		})
	}

	// Validate:
	// bucket 0: level=INFO sum=1, level=ERROR sum=1
	// bucket 60000: level=ERROR sum=1
	var b0Info, b0Err, b1Err int64
	for _, a := range got {
		switch a.bucket {
		case 0:
			if a.level == "INFO" {
				b0Info += a.sum
			} else if a.level == "ERROR" {
				b0Err += a.sum
			}
		case 60000:
			if a.level == "ERROR" {
				b1Err += a.sum
			}
		}
	}
	if b0Info != 1 || b0Err != 1 || b1Err != 1 {
		t.Fatalf("unexpected sums: b0(INFO)=%d b0(ERROR)=%d b1(ERROR)=%d; rows=%v", b0Info, b0Err, b1Err, got)
	}
}

func TestUnwrap(t *testing.T) {
	q := `avg_over_time({app="api"} |= "request" | json | unwrap latency_ms [1m])`
	ast, err := logql.FromLogQL(q)
	if err != nil {
		t.Fatalf("FromLogQL error: %v", err)
	}
	lplan, err := logql.CompileLog(ast)
	if err != nil {
		t.Fatalf("CompileLog error: %v", err)
	}
	if len(lplan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(lplan.Leaves))
	}
}

// 2) bytes_rate grouped by logfmt field (user)
// 2) bytes_rate grouped by logfmt field (user)
func TestRewrite_BytesRate_ByUser_Logfmt(t *testing.T) {
	db := openDuckDB(t)

	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Two buckets; ensure different message lengths per user to validate bytes aggregation.
	// lengths:
	//   "ts=5 user=bob msg=\"alpha\""   -> 25
	//   "ts=40 user=carol m=\"zz\""     -> 23    (shorter: use m= instead of msg=)
	//   "ts=65 user=bob msg=\"betaa\""  -> 26    (longer: extra 'a')
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(5*1000,   'web', 'ts=5 user=bob msg="alpha"'),
	(40*1000,  'web', 'ts=40 user=carol m="zz"'),
	(65*1000,  'web', 'ts=65 user=bob msg="betaa"');
	`)

	// LogQL: bytes_rate over 1m, sum by (user)
	q := `sum by (user) (bytes_rate({job="web"} | logfmt [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	step := time.Minute
	sql := be.ToWorkerSQL(step)
	sql = replaceWorkerPlaceholders(sql, 0, 120*1000)

	rows := queryAll(t, db, sql)

	type agg struct {
		bucket int64
		user   string
		sum    int64
	}
	var got []agg
	for _, r := range rows {
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			user:   s(r["user"]),
			sum:    i64(r["sum"]),
		})
	}

	// Expect:
	//   bucket 0 (0..60s): bob=25, carol=23
	//   bucket 60s..120s:  bob=26
	var b0Bob, b0Carol, b1Bob int64
	for _, a := range got {
		switch a.bucket {
		case 0:
			if a.user == "bob" {
				b0Bob += a.sum
			}
			if a.user == "carol" {
				b0Carol += a.sum
			}
		case 60000:
			if a.user == "bob" {
				b1Bob += a.sum
			}
		}
	}
	if b0Bob != 25 || b0Carol != 23 || b1Bob != 26 {
		t.Fatalf("unexpected byte sums: b0(bob)=%d b0(carol)=%d b1(bob)=%d; rows=%v", b0Bob, b0Carol, b1Bob, got)
	}
}

// 3) rate grouped by derived label via label_format (api)
func TestRewrite_Rate_ByDerivedLabel_LabelFormat(t *testing.T) {
	db := openDuckDB(t)

	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// bucket 0: response=ErrorBadGateway, OK
	// bucket 1: response=ErrorOops
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"response":"ErrorBadGateway","msg":"x"}'),
	(20*1000,  'svc', '{"response":"OK","msg":"y"}'),
	(80*1000,  'svc', '{"response":"ErrorOops","msg":"z"}');
	`)

	// Force projection of "response" before label_format so the current builder can
	// reference it inside the template (no semantic change; always-true filter).
	q := `sum by (api) (rate({job="svc"} | json | response=~".*" | label_format api=` +
		"`{{ if hasPrefix \"Error\" .response }}ERROR{{else}}{{.response}}{{end}}`" +
		` [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	step := time.Minute
	sql := be.ToWorkerSQL(step)
	sql = replaceWorkerPlaceholders(sql, 0, 120*1000)

	rows := queryAll(t, db, sql)

	type agg struct {
		bucket int64
		api    string
		sum    int64
	}
	var got []agg
	for _, r := range rows {
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			api:    s(r["api"]),
			sum:    i64(r["sum"]),
		})
	}

	// Expect grouping by derived api:
	// bucket 0: one ERROR, one OK -> sums 1 each
	// bucket 1: one ERROR -> sum 1
	var b0Err, b0OK, b1Err int64
	for _, a := range got {
		switch a.bucket {
		case 0:
			if a.api == "ERROR" {
				b0Err += a.sum
			}
			if a.api == "OK" {
				b0OK += a.sum
			}
		case 60000:
			if a.api == "ERROR" {
				b1Err += a.sum
			}
		}
	}
	if b0Err != 1 || b0OK != 1 || b1Err != 1 {
		t.Fatalf("unexpected sums: b0(ERROR)=%d b0(OK)=%d b1(ERROR)=%d; rows=%v", b0Err, b0OK, b1Err, got)
	}
}

func f64(v any) float64 {
	switch x := v.(type) {
	case float64:
		return x
	case float32:
		return float64(x)
	case int64:
		return float64(x)
	case int32:
		return float64(x)
	case int16:
		return float64(x)
	case int8:
		return float64(x)
	case int:
		return float64(x)
	case uint64:
		return float64(x)
	case uint32:
		return float64(x)
	case uint16:
		return float64(x)
	case uint8:
		return float64(x)
	case []byte:
		if n, err := strconv.ParseFloat(string(x), 64); err == nil {
			return n
		}
		return 0
	default:
		if n, err := strconv.ParseFloat(fmt.Sprintf("%v", x), 64); err == nil {
			return n
		}
		return 0
	}
}

func TestRewrite_Unwrap_Avg_JSON(t *testing.T) {
	db := openDuckDB(t)
	// Table must include exemplar defaults the LogQL pipeline always projects.
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Two 1m buckets: [0..60s) and [60..120s)
	// bucket 0: latency_ms = 100, 200   => avg = 150
	// bucket 1: latency_ms = 300        => avg = 300
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"latency_ms":"100","msg":"a"}'),
	(30*1000,  'svc', '{"latency_ms":"200","msg":"b"}'),
	(70*1000,  'svc', '{"latency_ms":"300","msg":"c"}');
	`)

	// LogQL: avg_over_time over 1m on an unwrapped numeric field from JSON
	q := `avg_over_time({job="svc"} | json | unwrap latency_ms [1m])`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	step := time.Minute
	replacedSql := replaceWorkerPlaceholders(be.ToWorkerSQL(step), 0, 120*1000)

	rows := queryAll(t, db, replacedSql)
	if len(rows) == 0 {
		t.Fatalf("no rows returned; sql=\n%s", replacedSql)
	}

	type agg struct {
		bucket int64
		avg    float64
	}

	// The aggregation column should be "avg" (consistent with other tests using "sum").
	// If your compiler names it differently, adjust the key here.
	var got []agg
	for _, r := range rows {
		avg := r["avg"]
		if avg == nil {
			// try a couple of common fallbacks just in case
			if v, ok := r["avg_over_time"]; ok {
				avg = v
			} else if v, ok := r["mean"]; ok {
				avg = v
			}
		}
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			avg:    f64(avg),
		})
	}

	var b0Avg, b1Avg float64
	for _, a := range got {
		switch a.bucket {
		case 0:
			b0Avg = a.avg
		case 60000:
			b1Avg = a.avg
		}
	}

	if b0Avg == 0 && b1Avg == 0 {
		t.Fatalf("unexpected zero avgs; rows=%v\nsql=\n%s", rows, replacedSql)
	}

	// Allow a tiny float slop
	const eps = 1e-9
	if !(b0Avg > 150.0-eps && b0Avg < 150.0+eps) || !(b1Avg > 300.0-eps && b1Avg < 300.0+eps) {
		t.Fatalf("unexpected avgs: bucket0=%v bucket1=%v; rows=%v", b0Avg, b1Avg, rows)
	}
}

// avg_over_time on an unwrapped numeric field extracted via logfmt
func TestRewrite_Unwrap_Avg_Logfmt(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Two 1m buckets: [0..60s), [60..120s)
	// bucket 0: latency_ms = 100, 200 => avg = 150
	// bucket 1: latency_ms = 300      => avg = 300
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', 'latency_ms=100 msg=a'),
	(30*1000,  'svc', 'latency_ms=200 msg=b'),
	(70*1000,  'svc', 'latency_ms=300 msg=c');
	`)

	q := `avg_over_time({job="svc"} | logfmt | unwrap latency_ms [1m])`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows returned; sql=\n%s", sql)
	}

	type agg struct {
		bucket int64
		avg    float64
	}
	var got []agg
	for _, r := range rows {
		v := r["avg"]
		if v == nil {
			if x, ok := r["avg_over_time"]; ok {
				v = x
			}
		}
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			avg:    f64(v),
		})
	}

	var b0, b1 float64
	for _, a := range got {
		switch a.bucket {
		case 0:
			b0 = a.avg
		case 60000:
			b1 = a.avg
		}
	}
	if b0 == 0 && b1 == 0 {
		t.Fatalf("unexpected zero avgs; rows=%v\nsql=\n%s", rows, sql)
	}
	const eps = 1e-9
	if !(b0 > 150.0-eps && b0 < 150.0+eps) || !(b1 > 300.0-eps && b1 < 300.0+eps) {
		t.Fatalf("unexpected avgs: bucket0=%v bucket1=%v; rows=%v", b0, b1, rows)
	}
}

// min_over_time on unwrap duration(...) from JSON (values converted to seconds)
func TestRewrite_Unwrap_Min_Duration_JSON(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// bucket 0: 100ms, 200ms -> min = 0.1
	// bucket 1: 300ms        -> min = 0.3
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"lat":"100ms","msg":"a"}'),
	(30*1000,  'svc', '{"lat":"200ms","msg":"b"}'),
	(70*1000,  'svc', '{"lat":"300ms","msg":"c"}');
	`)

	q := `min_over_time({job="svc"} | json | unwrap duration(lat) [1m])`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows returned; sql=\n%s", sql)
	}

	type agg struct {
		bucket int64
		min    float64
	}
	var got []agg
	for _, r := range rows {
		v := r["min"]
		if v == nil {
			if x, ok := r["min_over_time"]; ok {
				v = x
			}
		}
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			min:    f64(v),
		})
	}

	var b0, b1 float64
	for _, a := range got {
		switch a.bucket {
		case 0:
			b0 = a.min
		case 60000:
			b1 = a.min
		}
	}
	const eps = 1e-9
	if !(b0 > 0.1-eps && b0 < 0.1+eps) || !(b1 > 0.3-eps && b1 < 0.3+eps) {
		t.Fatalf("unexpected mins: bucket0=%v bucket1=%v; rows=%v", b0, b1, rows)
	}
}

// max_over_time on unwrap bytes(...) from logfmt (KB are decimal *1000)
func TestRewrite_Unwrap_Max_Bytes_Logfmt(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// bucket 0: 1KB, 2KB -> max = 2000
	// bucket 1: 3KB      -> max = 3000
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', 'size=1KB path=/a'),
	(30*1000,  'svc', 'size=2KB path=/b'),
	(70*1000,  'svc', 'size=3KB path=/c');
	`)

	q := `max_over_time({job="svc"} | logfmt | unwrap bytes(size) [1m])`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows returned; sql=\n%s", sql)
	}

	type agg struct {
		bucket int64
		max    float64
	}
	var got []agg
	for _, r := range rows {
		v := r["max"]
		if v == nil {
			if x, ok := r["max_over_time"]; ok {
				v = x
			}
		}
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			max:    f64(v),
		})
	}

	var b0, b1 float64
	for _, a := range got {
		switch a.bucket {
		case 0:
			b0 = a.max
		case 60000:
			b1 = a.max
		}
	}
	const eps = 1e-9
	if !(b0 > 2000.0-eps && b0 < 2000.0+eps) || !(b1 > 3000.0-eps && b1 < 3000.0+eps) {
		t.Fatalf("unexpected maxes: bucket0=%v bucket1=%v; rows=%v", b0, b1, rows)
	}
}

func TestRewrite_Unwrap_Max_Duration_Regexp(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Two 1m buckets: [0..60s) and [60..120s)
	// bucket 0 messages contain 2 ms and 4 ms -> max = 4 ms = 0.004 s
	// bucket 1 message contains 3 ms         -> max = 3 ms = 0.003 s
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'kafka', '[LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 111 in 2 ms.'),
	(30*1000,  'kafka', '[LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 222 in 4 ms.'),
	(70*1000,  'kafka', '[LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 333 in 3 ms.');
	`)

	// LogQL: max_over_time on an unwrapped duration captured by regexp
	// NOTE: unwrap duration(...) yields seconds as DOUBLE, so we expect 0.004 and 0.003.
	q := `max_over_time({job="kafka"} |= "Rolled new log segment" | regexp "in (?P<roll_dur>[0-9]+(?:\\.[0-9]+)?\\s*(?:ns|us|µs|ms|s|m|h))" | unwrap duration(roll_dur) [5m])`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows returned; sql=\n%s", sql)
	}

	type agg struct {
		bucket int64
		max    float64
	}
	var got []agg
	for _, r := range rows {
		v := r["max"]
		if v == nil {
			if x, ok := r["max_over_time"]; ok {
				v = x
			}
		}
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			max:    f64(v),
		})
	}

	var b0, b1 float64
	for _, a := range got {
		switch a.bucket {
		case 0:
			b0 = a.max
		case 60000:
			b1 = a.max
		}
	}

	const eps = 1e-9
	if !(b0 > 0.004-eps && b0 < 0.004+eps) || !(b1 > 0.003-eps && b1 < 0.003+eps) {
		t.Fatalf("unexpected maxes: bucket0=%v bucket1=%v; rows=%v\nsql=\n%s", b0, b1, rows, sql)
	}
}
