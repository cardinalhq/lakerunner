// Copyright (C) 2025 CardinalHQ, Inc
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as
// published by the Free Software Foundation, version 3.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program. If not, see <http://www.gnu.org/licenses/>.

package queryapi

import (
	"database/sql"
	"fmt"
	"github.com/cardinalhq/lakerunner/logql"
	"github.com/cardinalhq/lakerunner/promql"
	_ "github.com/marcboeker/go-duckdb/v2"

	"log/slog"
	"strconv"
	"strings"
	"testing"
	"time"
)

// -------- AGGREGATE tests ------------------------------------------------

// For worker SQL generated by BaseExpr.ToWorkerSQL(...)
// We need to replace {table}, {start}, {end}.
func replaceWorkerPlaceholders(sql string, start, end int64) string {
	sql = strings.ReplaceAll(sql, "{table}", "logs")
	sql = strings.ReplaceAll(sql, "{start}", fmt.Sprintf("%d", start))
	sql = strings.ReplaceAll(sql, "{end}", fmt.Sprintf("%d", end))
	return sql
}

// Map the LogLeafs back into BaseExpr leaves using the __leaf matcher.
func attachLeavesToPlan(t *testing.T, rr promql.RewriteResult, plan promql.QueryPlan) {
	t.Helper()
	for i := range plan.Leaves {
		be := &plan.Leaves[i] // mutate slice element
		found := false
		var kept []promql.LabelMatch
		for _, m := range be.Matchers {
			if m.Label == promql.LeafMatcher {
				leaf, ok := rr.Leaves[m.Value]
				if !ok {
					t.Fatalf("unknown __leaf id: %s", m.Value)
				}
				be.LogLeaf = leaf
				found = true
				continue // drop __leaf from matchers
			}
			kept = append(kept, m)
		}
		if !found {
			t.Fatalf("base expr missing __leaf matcher")
		}
		if be.LogLeaf.ID == "" {
			t.Fatalf("attached LogLeaf has empty ID")
		}
		be.Matchers = kept // strip __leaf matcher
	}
}

// End-to-end: LogQL -> CompileLog (leaves) -> RewriteToPromQL -> parse PromQL -> Compile
// -> attach leaves -> return the compiled plan (typically 1 leaf).
func compileMetricPlanFromLogQL(t *testing.T, q string) (promql.QueryPlan, promql.RewriteResult) {
	t.Helper()

	ast, err := logql.FromLogQL(q)
	if err != nil {
		t.Fatalf("FromLogQL error: %v", err)
	}
	lplan, err := logql.CompileLog(ast)
	if err != nil {
		t.Fatalf("CompileLog error: %v", err)
	}

	rr, err := promql.RewriteToPromQL(lplan.Root)
	if err != nil {
		t.Fatalf("RewriteToPromQL error: %v", err)
	}
	promExpr, err := promql.FromPromQL(rr.PromQL)
	if err != nil {
		t.Fatalf("parse rewritten PromQL: %v (sql=%s)", err, rr.PromQL)
	}
	plan, err := promql.Compile(promExpr)
	if err != nil {
		t.Fatalf("compile rewritten PromQL: %v", err)
	}
	attachLeavesToPlan(t, rr, plan)
	return plan, rr
}

// pull typed values (like in your existing tests)
func s(v any) string {
	switch x := v.(type) {
	case nil:
		return ""
	case string:
		return x
	case []byte:
		return string(x)
	default:
		return fmt.Sprintf("%v", x)
	}
}

func i64(v any) int64 {
	switch x := v.(type) {
	case int64:
		return x
	case int32:
		return int64(x)
	case int16:
		return int64(x)
	case int8:
		return int64(x)
	case int:
		return int64(x)
	case uint64:
		return int64(x)
	case uint32:
		return int64(x)
	case uint16:
		return int64(x)
	case uint8:
		return int64(x)
	case float64:
		return int64(x)
	case float32:
		return int64(x)
	case []byte:
		// try parsing if driver returned numeric as bytes
		if n, err := strconv.ParseInt(string(x), 10, 64); err == nil {
			return n
		}
		return 0
	default:
		// last resort: string then parse
		if n, err := strconv.ParseInt(fmt.Sprintf("%v", x), 10, 64); err == nil {
			return n
		}
		return 0
	}
}

func openDuckDB(t *testing.T) *sql.DB {
	db, err := sql.Open("duckdb", "")
	if err != nil {
		slog.Error("Error opening duckdb for local parquet", "error", err.Error())
		t.Fatalf("open duckdb: %v", err)
	}
	t.Cleanup(func() { _ = db.Close() })
	return db
}

func mustExec(t *testing.T, db *sql.DB, q string, args ...any) {
	t.Helper()
	if _, err := db.Exec(q, args...); err != nil {
		t.Fatalf("exec failed: %v\nsql:\n%s", err, q)
	}
}

type rowmap map[string]any

func queryAll(t *testing.T, db *sql.DB, q string) []rowmap {
	t.Helper()
	rows, err := db.Query(q)
	if err != nil {
		t.Fatalf("query failed: %v\nsql:\n%s", err, q)
	}
	defer rows.Close()

	cols, err := rows.Columns()
	if err != nil {
		t.Fatalf("columns failed: %v", err)
	}

	var out []rowmap
	for rows.Next() {
		raw := make([]any, len(cols))
		ptrs := make([]any, len(cols))
		for i := range raw {
			ptrs[i] = &raw[i]
		}
		if err := rows.Scan(ptrs...); err != nil {
			t.Fatalf("scan failed: %v", err)
		}
		m := make(rowmap, len(cols))
		for i, c := range cols {
			m[c] = raw[i]
		}
		out = append(out, m)
	}
	if err := rows.Err(); err != nil {
		t.Fatalf("rows err: %v", err)
	}
	return out
}

// --- Tests -----------------------------------------------------------------

// 1) count_over_time grouped by json field (level)
func TestRewrite_CountOverTime_ByLevel_JSON(t *testing.T) {
	db := openDuckDB(t)
	// Table must include exemplar defaults the LogQL pipeline always projects.
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Data: 3 events in two 1m buckets (0-60s, 60-120s)
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"level":"INFO","user":"alice","msg":"a"}'),
	(30*1000,  'svc', '{"level":"ERROR","user":"bob","msg":"b"}'),
	(70*1000,  'svc', '{"level":"ERROR","user":"carol","msg":"c"}');
	`)

	// LogQL: count_over_time over 1m, sum by (level)
	q := `sum by (level) (count_over_time({job="svc"} | json [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	// Worker SQL for step=1m
	step := time.Minute
	sql := be.ToWorkerSQL(step)

	// Concretize placeholders for [0, 120s)
	sql = replaceWorkerPlaceholders(sql, 0, 120*1000)

	rows := queryAll(t, db, sql)

	type agg struct {
		bucket int64
		level  string
		sum    int64
	}
	var got []agg
	for _, r := range rows {
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			level:  s(r["level"]),
			sum:    i64(r["sum"]),
		})
	}

	// Validate:
	// bucket 0: level=INFO sum=1, level=ERROR sum=1
	// bucket 60000: level=ERROR sum=1
	var b0Info, b0Err, b1Err int64
	for _, a := range got {
		switch a.bucket {
		case 0:
			if a.level == "INFO" {
				b0Info += a.sum
			} else if a.level == "ERROR" {
				b0Err += a.sum
			}
		case 60000:
			if a.level == "ERROR" {
				b1Err += a.sum
			}
		}
	}
	if b0Info != 1 || b0Err != 1 || b1Err != 1 {
		t.Fatalf("unexpected sums: b0(INFO)=%d b0(ERROR)=%d b1(ERROR)=%d; rows=%v", b0Info, b0Err, b1Err, got)
	}
}

// 2) bytes_rate grouped by logfmt field (user)
// 2) bytes_rate grouped by logfmt field (user)
func TestRewrite_BytesRate_ByUser_Logfmt(t *testing.T) {
	db := openDuckDB(t)

	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Two buckets; ensure different message lengths per user to validate bytes aggregation.
	// lengths:
	//   "ts=5 user=bob msg=\"alpha\""   -> 25
	//   "ts=40 user=carol m=\"zz\""     -> 23    (shorter: use m= instead of msg=)
	//   "ts=65 user=bob msg=\"betaa\""  -> 26    (longer: extra 'a')
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(5*1000,   'web', 'ts=5 user=bob msg="alpha"'),
	(40*1000,  'web', 'ts=40 user=carol m="zz"'),
	(65*1000,  'web', 'ts=65 user=bob msg="betaa"');
	`)

	// LogQL: bytes_rate over 1m, sum by (user)
	q := `sum by (user) (bytes_rate({job="web"} | logfmt [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	step := time.Minute
	sql := be.ToWorkerSQL(step)
	sql = replaceWorkerPlaceholders(sql, 0, 120*1000)

	rows := queryAll(t, db, sql)

	type agg struct {
		bucket int64
		user   string
		sum    int64
	}
	var got []agg
	for _, r := range rows {
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			user:   s(r["user"]),
			sum:    i64(r["sum"]),
		})
	}

	// Expect:
	//   bucket 0 (0..60s): bob=25, carol=23
	//   bucket 60s..120s:  bob=26
	var b0Bob, b0Carol, b1Bob int64
	for _, a := range got {
		switch a.bucket {
		case 0:
			if a.user == "bob" {
				b0Bob += a.sum
			}
			if a.user == "carol" {
				b0Carol += a.sum
			}
		case 60000:
			if a.user == "bob" {
				b1Bob += a.sum
			}
		}
	}
	if b0Bob != 25 || b0Carol != 23 || b1Bob != 26 {
		t.Fatalf("unexpected byte sums: b0(bob)=%d b0(carol)=%d b1(bob)=%d; rows=%v", b0Bob, b0Carol, b1Bob, got)
	}
}

// 3) rate grouped by derived label via label_format (api)
func TestRewrite_Rate_ByDerivedLabel_LabelFormat(t *testing.T) {
	db := openDuckDB(t)

	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// bucket 0: response=ErrorBadGateway, OK
	// bucket 1: response=ErrorOops
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"response":"ErrorBadGateway","msg":"x"}'),
	(20*1000,  'svc', '{"response":"OK","msg":"y"}'),
	(80*1000,  'svc', '{"response":"ErrorOops","msg":"z"}');
	`)

	// Force projection of "response" before label_format so the current builder can
	// reference it inside the template (no semantic change; always-true filter).
	q := `sum by (api) (rate({job="svc"} | json | response=~".*" | label_format api=` +
		"`{{ if hasPrefix \"Error\" .response }}ERROR{{else}}{{.response}}{{end}}`" +
		` [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	step := time.Minute
	sql := be.ToWorkerSQL(step)
	sql = replaceWorkerPlaceholders(sql, 0, 120*1000)

	rows := queryAll(t, db, sql)

	type agg struct {
		bucket int64
		api    string
		sum    int64
	}
	var got []agg
	for _, r := range rows {
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			api:    s(r["api"]),
			sum:    i64(r["sum"]),
		})
	}

	// Expect grouping by derived api:
	// bucket 0: one ERROR, one OK -> sums 1 each
	// bucket 1: one ERROR -> sum 1
	var b0Err, b0OK, b1Err int64
	for _, a := range got {
		switch a.bucket {
		case 0:
			if a.api == "ERROR" {
				b0Err += a.sum
			}
			if a.api == "OK" {
				b0OK += a.sum
			}
		case 60000:
			if a.api == "ERROR" {
				b1Err += a.sum
			}
		}
	}
	if b0Err != 1 || b0OK != 1 || b1Err != 1 {
		t.Fatalf("unexpected sums: b0(ERROR)=%d b0(OK)=%d b1(ERROR)=%d; rows=%v", b0Err, b0OK, b1Err, got)
	}
}
