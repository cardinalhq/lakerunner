// Copyright (C) 2025 CardinalHQ, Inc
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as
// published by the Free Software Foundation, version 3.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program. If not, see <http://www.gnu.org/licenses/>.

package queryapi

import (
	"database/sql"
	"fmt"
	"github.com/cardinalhq/lakerunner/logql"
	"github.com/cardinalhq/lakerunner/promql"
	_ "github.com/marcboeker/go-duckdb/v2"

	"log/slog"
	"strconv"
	"strings"
	"testing"
	"time"
)

// -------- AGGREGATE tests ------------------------------------------------

// For worker SQL generated by BaseExpr.ToWorkerSQL(...)
// We need to replace {table}, {start}, {end}.
func replaceWorkerPlaceholders(sql string, start, end int64) string {
	sql = strings.ReplaceAll(sql, "{table}", "logs")
	sql = strings.ReplaceAll(sql, "{start}", fmt.Sprintf("%d", start))
	sql = strings.ReplaceAll(sql, "{end}", fmt.Sprintf("%d", end))
	return sql
}

// Map the LogLeafs back into BaseExpr leaves using the __leaf matcher.
func attachLeavesToPlan(t *testing.T, rr promql.RewriteResult, plan promql.QueryPlan) {
	t.Helper()
	for i := range plan.Leaves {
		be := &plan.Leaves[i] // mutate slice element
		found := false
		var kept []promql.LabelMatch
		for _, m := range be.Matchers {
			if m.Label == promql.LeafMatcher {
				leaf, ok := rr.Leaves[m.Value]
				if !ok {
					t.Fatalf("unknown __leaf id: %s", m.Value)
				}
				be.LogLeaf = &leaf
				found = true
				continue // drop __leaf from matchers
			}
			kept = append(kept, m)
		}
		if !found {
			t.Fatalf("base expr missing __leaf matcher")
		}
		if be.LogLeaf.ID == "" {
			t.Fatalf("attached LogLeaf has empty ID")
		}
		be.Matchers = kept // strip __leaf matcher
	}
}

// End-to-end: LogQL -> CompileLog (leaves) -> RewriteToPromQL -> parse PromQL -> Compile
// -> attach leaves -> return the compiled plan (typically 1 leaf).
func compileMetricPlanFromLogQL(t *testing.T, q string) (promql.QueryPlan, promql.RewriteResult) {
	t.Helper()

	ast, err := logql.FromLogQL(q)
	if err != nil {
		t.Fatalf("FromLogQL error: %v", err)
	}
	lplan, err := logql.CompileLog(ast)
	if err != nil {
		t.Fatalf("CompileLog error: %v", err)
	}

	rr, err := promql.RewriteToPromQL(lplan.Root)
	if err != nil {
		t.Fatalf("RewriteToPromQL error: %v", err)
	}
	promExpr, err := promql.FromPromQL(rr.PromQL)
	if err != nil {
		t.Fatalf("parse rewritten PromQL: %v (sql=%s)", err, rr.PromQL)
	}
	plan, err := promql.Compile(promExpr)
	if err != nil {
		t.Fatalf("compile rewritten PromQL: %v", err)
	}
	attachLeavesToPlan(t, rr, plan)
	return plan, rr
}

// pull typed values (like in your existing tests)
func s(v any) string {
	switch x := v.(type) {
	case nil:
		return ""
	case string:
		return x
	case []byte:
		return string(x)
	default:
		return fmt.Sprintf("%v", x)
	}
}

func i64(v any) int64 {
	switch x := v.(type) {
	case int64:
		return x
	case int32:
		return int64(x)
	case int16:
		return int64(x)
	case int8:
		return int64(x)
	case int:
		return int64(x)
	case uint64:
		return int64(x)
	case uint32:
		return int64(x)
	case uint16:
		return int64(x)
	case uint8:
		return int64(x)
	case float64:
		return int64(x)
	case float32:
		return int64(x)
	case []byte:
		// try parsing if driver returned numeric as bytes
		if n, err := strconv.ParseInt(string(x), 10, 64); err == nil {
			return n
		}
		return 0
	default:
		// last resort: string then parse
		if n, err := strconv.ParseInt(fmt.Sprintf("%v", x), 10, 64); err == nil {
			return n
		}
		return 0
	}
}

func openDuckDB(t *testing.T) *sql.DB {
	db, err := sql.Open("duckdb", "")
	if err != nil {
		slog.Error("Error opening duckdb for local parquet", "error", err.Error())
		t.Fatalf("open duckdb: %v", err)
	}
	t.Cleanup(func() { _ = db.Close() })
	return db
}

func mustExec(t *testing.T, db *sql.DB, q string, args ...any) {
	t.Helper()
	if _, err := db.Exec(q, args...); err != nil {
		t.Fatalf("exec failed: %v\nsql:\n%s", err, q)
	}
}

type rowmap map[string]any

func queryAll(t *testing.T, db *sql.DB, q string) []rowmap {
	t.Helper()
	rows, err := db.Query(q)
	if err != nil {
		t.Fatalf("query failed: %v\nsql:\n%s", err, q)
	}
	defer rows.Close()

	cols, err := rows.Columns()
	if err != nil {
		t.Fatalf("columns failed: %v", err)
	}

	var out []rowmap
	for rows.Next() {
		raw := make([]any, len(cols))
		ptrs := make([]any, len(cols))
		for i := range raw {
			ptrs[i] = &raw[i]
		}
		if err := rows.Scan(ptrs...); err != nil {
			t.Fatalf("scan failed: %v", err)
		}
		m := make(rowmap, len(cols))
		for i, c := range cols {
			m[c] = raw[i]
		}
		out = append(out, m)
	}
	if err := rows.Err(); err != nil {
		t.Fatalf("rows err: %v", err)
	}
	return out
}

// --- Tests -----------------------------------------------------------------

// 1) count_over_time grouped by json field (level)
func TestRewrite_CountOverTime_ByLevel_JSON(t *testing.T) {
	db := openDuckDB(t)
	// Table must include exemplar defaults the LogQL pipeline always projects.
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Data: 3 events in two 1m buckets (0-60s, 60-120s)
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"level":"INFO","user":"alice","msg":"a"}'),
	(30*1000,  'svc', '{"level":"ERROR","user":"bob","msg":"b"}'),
	(70*1000,  'svc', '{"level":"ERROR","user":"carol","msg":"c"}');
	`)

	// LogQL: count_over_time over 1m, sum by (level)
	q := `sum by (level) (count_over_time({job="svc"} | json [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	// Worker SQL for step=1m
	step := time.Minute
	sql := be.ToWorkerSQL(step)

	// Concretize placeholders for [0, 120s)
	sql = replaceWorkerPlaceholders(sql, 0, 120*1000)

	rows := queryAll(t, db, sql)

	type agg struct {
		bucket int64
		level  string
		sum    int64
	}
	var got []agg
	for _, r := range rows {
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			level:  s(r["level"]),
			sum:    i64(r["sum"]),
		})
	}

	// Validate:
	// bucket 0: level=INFO sum=1, level=ERROR sum=1
	// bucket 60000: level=ERROR sum=1
	var b0Info, b0Err, b1Err int64
	for _, a := range got {
		switch a.bucket {
		case 0:
			if a.level == "INFO" {
				b0Info += a.sum
			} else if a.level == "ERROR" {
				b0Err += a.sum
			}
		case 60000:
			if a.level == "ERROR" {
				b1Err += a.sum
			}
		}
	}
	if b0Info != 1 || b0Err != 1 || b1Err != 1 {
		t.Fatalf("unexpected sums: b0(INFO)=%d b0(ERROR)=%d b1(ERROR)=%d; rows=%v", b0Info, b0Err, b1Err, got)
	}
}

func TestUnwrap(t *testing.T) {
	q := `avg_over_time({app="api"} |= "request" | json | unwrap latency_ms [1m])`
	ast, err := logql.FromLogQL(q)
	if err != nil {
		t.Fatalf("FromLogQL error: %v", err)
	}
	lplan, err := logql.CompileLog(ast)
	if err != nil {
		t.Fatalf("CompileLog error: %v", err)
	}
	if len(lplan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(lplan.Leaves))
	}
}

// 2) bytes_rate grouped by logfmt field (user)
// 2) bytes_rate grouped by logfmt field (user)
func TestRewrite_BytesRate_ByUser_Logfmt(t *testing.T) {
	db := openDuckDB(t)

	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Two buckets; ensure different message lengths per user to validate bytes aggregation.
	// lengths:
	//   "ts=5 user=bob msg=\"alpha\""   -> 25
	//   "ts=40 user=carol m=\"zz\""     -> 23    (shorter: use m= instead of msg=)
	//   "ts=65 user=bob msg=\"betaa\""  -> 26    (longer: extra 'a')
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(5*1000,   'web', 'ts=5 user=bob msg="alpha"'),
	(40*1000,  'web', 'ts=40 user=carol m="zz"'),
	(65*1000,  'web', 'ts=65 user=bob msg="betaa"');
	`)

	// LogQL: bytes_rate over 1m, sum by (user)
	q := `sum by (user) (bytes_rate({job="web"} | logfmt [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	step := time.Minute
	sql := be.ToWorkerSQL(step)
	sql = replaceWorkerPlaceholders(sql, 0, 120*1000)

	rows := queryAll(t, db, sql)

	type agg struct {
		bucket int64
		user   string
		sum    int64
	}
	var got []agg
	for _, r := range rows {
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			user:   s(r["user"]),
			sum:    i64(r["sum"]),
		})
	}

	// Expect:
	//   bucket 0 (0..60s): bob=25, carol=23
	//   bucket 60s..120s:  bob=26
	var b0Bob, b0Carol, b1Bob int64
	for _, a := range got {
		switch a.bucket {
		case 0:
			if a.user == "bob" {
				b0Bob += a.sum
			}
			if a.user == "carol" {
				b0Carol += a.sum
			}
		case 60000:
			if a.user == "bob" {
				b1Bob += a.sum
			}
		}
	}
	if b0Bob != 25 || b0Carol != 23 || b1Bob != 26 {
		t.Fatalf("unexpected byte sums: b0(bob)=%d b0(carol)=%d b1(bob)=%d; rows=%v", b0Bob, b0Carol, b1Bob, got)
	}
}

// 3) rate grouped by derived label via label_format (api)
func TestRewrite_Rate_ByDerivedLabel_LabelFormat(t *testing.T) {
	db := openDuckDB(t)

	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// bucket 0: response=ErrorBadGateway, OK
	// bucket 1: response=ErrorOops
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"response":"ErrorBadGateway","msg":"x"}'),
	(20*1000,  'svc', '{"response":"OK","msg":"y"}'),
	(80*1000,  'svc', '{"response":"ErrorOops","msg":"z"}');
	`)

	// Force projection of "response" before label_format so the current builder can
	// reference it inside the template (no semantic change; always-true filter).
	q := `sum by (api) (rate({job="svc"} | json | response=~".*" | label_format api=` +
		"`{{ if hasPrefix \"Error\" .response }}ERROR{{else}}{{.response}}{{end}}`" +
		` [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	step := time.Minute
	sql := be.ToWorkerSQL(step)
	sql = replaceWorkerPlaceholders(sql, 0, 120*1000)

	rows := queryAll(t, db, sql)

	type agg struct {
		bucket int64
		api    string
		sum    int64
	}
	var got []agg
	for _, r := range rows {
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			api:    s(r["api"]),
			sum:    i64(r["sum"]),
		})
	}

	// Expect grouping by derived api:
	// bucket 0: one ERROR, one OK -> sums 1 each
	// bucket 1: one ERROR -> sum 1
	var b0Err, b0OK, b1Err int64
	for _, a := range got {
		switch a.bucket {
		case 0:
			if a.api == "ERROR" {
				b0Err += a.sum
			}
			if a.api == "OK" {
				b0OK += a.sum
			}
		case 60000:
			if a.api == "ERROR" {
				b1Err += a.sum
			}
		}
	}
	if b0Err != 1 || b0OK != 1 || b1Err != 1 {
		t.Fatalf("unexpected sums: b0(ERROR)=%d b0(OK)=%d b1(ERROR)=%d; rows=%v", b0Err, b0OK, b1Err, got)
	}
}

func f64(v any) float64 {
	switch x := v.(type) {
	case float64:
		return x
	case float32:
		return float64(x)
	case int64:
		return float64(x)
	case int32:
		return float64(x)
	case int16:
		return float64(x)
	case int8:
		return float64(x)
	case int:
		return float64(x)
	case uint64:
		return float64(x)
	case uint32:
		return float64(x)
	case uint16:
		return float64(x)
	case uint8:
		return float64(x)
	case []byte:
		if n, err := strconv.ParseFloat(string(x), 64); err == nil {
			return n
		}
		return 0
	default:
		if n, err := strconv.ParseFloat(fmt.Sprintf("%v", x), 64); err == nil {
			return n
		}
		return 0
	}
}

func TestRewrite_Unwrap_Avg_JSON(t *testing.T) {
	db := openDuckDB(t)
	// Table must include exemplar defaults the LogQL pipeline always projects.
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Two 1m buckets: [0..60s) and [60..120s)
	// bucket 0: latency_ms = 100, 200   => avg = 150
	// bucket 1: latency_ms = 300        => avg = 300
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"latency_ms":"100","msg":"a"}'),
	(30*1000,  'svc', '{"latency_ms":"200","msg":"b"}'),
	(70*1000,  'svc', '{"latency_ms":"300","msg":"c"}');
	`)

	// LogQL: avg_over_time over 1m on an unwrapped numeric field from JSON
	q := `avg_over_time({job="svc"} | json | unwrap latency_ms [1m])`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	step := time.Minute
	replacedSql := replaceWorkerPlaceholders(be.ToWorkerSQL(step), 0, 120*1000)

	rows := queryAll(t, db, replacedSql)
	if len(rows) == 0 {
		t.Fatalf("no rows returned; sql=\n%s", replacedSql)
	}

	type agg struct {
		bucket int64
		avg    float64
	}

	// The aggregation column should be "avg" (consistent with other tests using "sum").
	// If your compiler names it differently, adjust the key here.
	var got []agg
	for _, r := range rows {
		avg := r["avg"]
		if avg == nil {
			// try a couple of common fallbacks just in case
			if v, ok := r["avg_over_time"]; ok {
				avg = v
			} else if v, ok := r["mean"]; ok {
				avg = v
			}
		}
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			avg:    f64(avg),
		})
	}

	var b0Avg, b1Avg float64
	for _, a := range got {
		switch a.bucket {
		case 0:
			b0Avg = a.avg
		case 60000:
			b1Avg = a.avg
		}
	}

	if b0Avg == 0 && b1Avg == 0 {
		t.Fatalf("unexpected zero avgs; rows=%v\nsql=\n%s", rows, replacedSql)
	}

	// Allow a tiny float slop
	const eps = 1e-9
	if !(b0Avg > 150.0-eps && b0Avg < 150.0+eps) || !(b1Avg > 300.0-eps && b1Avg < 300.0+eps) {
		t.Fatalf("unexpected avgs: bucket0=%v bucket1=%v; rows=%v", b0Avg, b1Avg, rows)
	}
}

// avg_over_time on an unwrapped numeric field extracted via logfmt
func TestRewrite_Unwrap_Avg_Logfmt(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Two 1m buckets: [0..60s), [60..120s)
	// bucket 0: latency_ms = 100, 200 => avg = 150
	// bucket 1: latency_ms = 300      => avg = 300
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', 'latency_ms=100 msg=a'),
	(30*1000,  'svc', 'latency_ms=200 msg=b'),
	(70*1000,  'svc', 'latency_ms=300 msg=c');
	`)

	q := `avg_over_time({job="svc"} | logfmt | unwrap latency_ms [1m])`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows returned; sql=\n%s", sql)
	}

	type agg struct {
		bucket int64
		avg    float64
	}
	var got []agg
	for _, r := range rows {
		v := r["avg"]
		if v == nil {
			if x, ok := r["avg_over_time"]; ok {
				v = x
			}
		}
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			avg:    f64(v),
		})
	}

	var b0, b1 float64
	for _, a := range got {
		switch a.bucket {
		case 0:
			b0 = a.avg
		case 60000:
			b1 = a.avg
		}
	}
	if b0 == 0 && b1 == 0 {
		t.Fatalf("unexpected zero avgs; rows=%v\nsql=\n%s", rows, sql)
	}
	const eps = 1e-9
	if !(b0 > 150.0-eps && b0 < 150.0+eps) || !(b1 > 300.0-eps && b1 < 300.0+eps) {
		t.Fatalf("unexpected avgs: bucket0=%v bucket1=%v; rows=%v", b0, b1, rows)
	}
}

// min_over_time on unwrap duration(...) from JSON (values converted to seconds)
func TestRewrite_Unwrap_Min_Duration_JSON(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// bucket 0: 100ms, 200ms -> min = 0.1
	// bucket 1: 300ms        -> min = 0.3
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"lat":"100ms","msg":"a"}'),
	(30*1000,  'svc', '{"lat":"200ms","msg":"b"}'),
	(70*1000,  'svc', '{"lat":"300ms","msg":"c"}');
	`)

	q := `min_over_time({job="svc"} | json | unwrap duration(lat) [1m])`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows returned; sql=\n%s", sql)
	}

	type agg struct {
		bucket int64
		min    float64
	}
	var got []agg
	for _, r := range rows {
		v := r["min"]
		if v == nil {
			if x, ok := r["min_over_time"]; ok {
				v = x
			}
		}
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			min:    f64(v),
		})
	}

	var b0, b1 float64
	for _, a := range got {
		switch a.bucket {
		case 0:
			b0 = a.min
		case 60000:
			b1 = a.min
		}
	}
	const eps = 1e-9
	if !(b0 > 0.1-eps && b0 < 0.1+eps) || !(b1 > 0.3-eps && b1 < 0.3+eps) {
		t.Fatalf("unexpected mins: bucket0=%v bucket1=%v; rows=%v", b0, b1, rows)
	}
}

// max_over_time on unwrap bytes(...) from logfmt (KB are decimal *1000)
func TestRewrite_Unwrap_Max_Bytes_Logfmt(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// bucket 0: 1KB, 2KB -> max = 2000
	// bucket 1: 3KB      -> max = 3000
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', 'size=1KB path=/a'),
	(30*1000,  'svc', 'size=2KB path=/b'),
	(70*1000,  'svc', 'size=3KB path=/c');
	`)

	q := `max_over_time({job="svc"} | logfmt | unwrap bytes(size) [1m])`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows returned; sql=\n%s", sql)
	}

	type agg struct {
		bucket int64
		max    float64
	}
	var got []agg
	for _, r := range rows {
		v := r["max"]
		if v == nil {
			if x, ok := r["max_over_time"]; ok {
				v = x
			}
		}
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			max:    f64(v),
		})
	}

	var b0, b1 float64
	for _, a := range got {
		switch a.bucket {
		case 0:
			b0 = a.max
		case 60000:
			b1 = a.max
		}
	}
	const eps = 1e-9
	if !(b0 > 2000.0-eps && b0 < 2000.0+eps) || !(b1 > 3000.0-eps && b1 < 3000.0+eps) {
		t.Fatalf("unexpected maxes: bucket0=%v bucket1=%v; rows=%v", b0, b1, rows)
	}
}

func TestRewrite_Unwrap_Max_Duration_Regexp(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Two 1m buckets: [0..60s) and [60..120s)
	// bucket 0 messages contain 2 ms and 4 ms -> max = 4 ms = 0.004 s
	// bucket 1 message contains 3 ms         -> max = 3 ms = 0.003 s
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'kafka', '[LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 111 in 2 ms.'),
	(30*1000,  'kafka', '[LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 222 in 4 ms.'),
	(70*1000,  'kafka', '[LocalLog partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Rolled new log segment at offset 333 in 3 ms.');
	`)

	// LogQL: max_over_time on an unwrapped duration captured by regexp
	// NOTE: unwrap duration(...) yields seconds as DOUBLE, so we expect 0.004 and 0.003.
	q := `max_over_time({job="kafka"} |= "Rolled new log segment" | regexp "in (?P<roll_dur>[0-9]+(?:\\.[0-9]+)?\\s*(?:ns|us|Âµs|ms|s|m|h))" | unwrap duration(roll_dur) [5m])`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows returned; sql=\n%s", sql)
	}

	type agg struct {
		bucket int64
		max    float64
	}
	var got []agg
	for _, r := range rows {
		v := r["max"]
		if v == nil {
			if x, ok := r["max_over_time"]; ok {
				v = x
			}
		}
		got = append(got, agg{
			bucket: i64(r["bucket_ts"]),
			max:    f64(v),
		})
	}

	var b0, b1 float64
	for _, a := range got {
		switch a.bucket {
		case 0:
			b0 = a.max
		case 60000:
			b1 = a.max
		}
	}

	const eps = 1e-9
	if !(b0 > 0.004-eps && b0 < 0.004+eps) || !(b1 > 0.003-eps && b1 < 0.003+eps) {
		t.Fatalf("unexpected maxes: bucket0=%v bucket1=%v; rows=%v\nsql=\n%s", b0, b1, rows, sql)
	}
}

func TestRewrite_SumByPod_Rate_ServiceFilter(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		"_cardinalhq.message"     TEXT,
		"resource.service.name"   TEXT,
		"resource.k8s.pod.name"   TEXT,
		"log.source"              TEXT
	);`)

	// Two 1m buckets: [0..60s) and [60..120s)
	// We create two streams per pod (log.source = 'a' | 'b') so sum by(pod) actually sums.
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp","_cardinalhq.message","resource.service.name","resource.k8s.pod.name","log.source") VALUES
	-- Pod A, bucket 0 (3)
	(10*1000,  'evt', 'api-gateway', 'api-7f', 'a'),
	(20*1000,  'evt', 'api-gateway', 'api-7f', 'b'),
	(40*1000,  'evt', 'api-gateway', 'api-7f', 'b'),

	-- Pod A, bucket 1 (6)
	(65*1000,  'evt', 'api-gateway', 'api-7f', 'a'),
	(70*1000,  'evt', 'api-gateway', 'api-7f', 'a'),
	(80*1000,  'evt', 'api-gateway', 'api-7f', 'b'),
	(90*1000,  'evt', 'api-gateway', 'api-7f', 'b'),
	(100*1000, 'evt', 'api-gateway', 'api-7f', 'b'),
	(110*1000, 'evt', 'api-gateway', 'api-7f', 'a'),

	-- Pod B, bucket 0 (2)
	(15*1000,  'evt', 'api-gateway', 'api-9x', 'a'),
	(55*1000,  'evt', 'api-gateway', 'api-9x', 'b'),

	-- Pod B, bucket 1 (1)
	(75*1000,  'evt', 'api-gateway', 'api-9x', 'a');
	`)

	// Worker SQL for: sum by (resource_k8s_pod_name)(rate({resource_service_name="api-gateway"}[5m]))
	// NOTE: Worker returns SUMs per bucket; top-level query-api converts to rate later.
	q := `sum by (resource_k8s_pod_name)(rate({resource_service_name="api-gateway"}[5m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows returned; sql=\n%s", sql)
	}

	// Pull the numeric value (worker should expose it as "sum").
	getVal := func(r map[string]any) float64 {
		if v, ok := r["sum"]; ok && v != nil {
			return f64(v)
		}
		if v, ok := r["value"]; ok && v != nil {
			return f64(v)
		}
		for k, v := range r { // fallback for variant aliases
			if strings.HasPrefix(k, "sum_") {
				return f64(v)
			}
		}
		return f64(nil)
	}

	type key struct {
		bucket int64
		pod    string
	}
	got := map[key]float64{}
	for _, r := range rows {
		b := i64(r["bucket_ts"])
		pod := ""
		if v, ok := r[`resource.k8s.pod.name`]; ok && v != nil {
			pod = fmt.Sprint(v)
		} else if v, ok := r[`resource_k8s_pod_name`]; ok && v != nil {
			pod = fmt.Sprint(v)
		} else {
			t.Fatalf("missing pod label column in row: %v\nsql=\n%s", r, sql)
		}
		got[key{bucket: b, pod: pod}] = getVal(r)
	}

	const (
		b0 = int64(0)
		b1 = int64(60000)
	)
	// EXPECT COUNTS (worker sum), NOT RATES. query-api will divide by the range later.
	exp := map[key]float64{
		{bucket: b0, pod: "api-7f"}: 3,
		{bucket: b1, pod: "api-7f"}: 6,
		{bucket: b0, pod: "api-9x"}: 2,
		{bucket: b1, pod: "api-9x"}: 1,
	}

	for k, want := range exp {
		gotv, ok := got[k]
		if !ok {
			t.Fatalf("missing bucket/pod %v in results; rows=%v\nsql=\n%s", k, rows, sql)
		}
		if gotv != want {
			t.Fatalf("unexpected sum for %v: got=%v want=%v; rows=%v\nsql=\n%s", k, gotv, want, rows, sql)
		}
	}
}

// --- label_format -----------------------------------------------------------
// Group by a label created via label_format: svc = {{ .resource.service.name }}
func TestRewrite_SumBy_LabelFormat_Service(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		"_cardinalhq.message"     TEXT,
		"resource.service.name"   TEXT
	);`)

	// Two buckets [0..60s), [60..120s)
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp","_cardinalhq.message","resource.service.name") VALUES
	(10*1000,  'm', 'svc-a'),
	(20*1000,  'm', 'svc-a'),
	(40*1000,  'm', 'svc-b'),

	(70*1000,  'm', 'svc-a'),
	(80*1000,  'm', 'svc-b'),
	(90*1000,  'm', 'svc-b');`)

	// Group by the label created by label_format; template references a dotted base column.
	q := "sum by (svc)(count_over_time({resource_service_name=~\"svc-.*\"} | label_format svc=`{{ .resource.service.name }}` [1m]))"

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows; sql=\n%s", sql)
	}

	type key struct {
		bucket int64
		svc    string
	}
	got := map[key]int64{}
	for _, r := range rows {
		b := i64(r["bucket_ts"])
		svc := s(r["svc"])
		sum := i64(r["sum"])
		got[key{b, svc}] = sum
	}

	exp := map[key]int64{
		{0, "svc-a"}:     2,
		{0, "svc-b"}:     1,
		{60000, "svc-a"}: 1,
		{60000, "svc-b"}: 2,
	}
	for k, want := range exp {
		if got[k] != want {
			t.Fatalf("unexpected sum for %v: got=%v want=%v; rows=%v\nsql=\n%s", k, got[k], want, rows, sql)
		}
	}
}

// --- json -------------------------------------------------------------------
// Group by JSON-extracted label: user
func TestRewrite_SumBy_JSON_User(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Two buckets:
	// bucket 0: users alice(2), bob(1)
	// bucket 1: users alice(1), carol(1)
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"user":"alice","msg":"a"}'),
	(25*1000,  'svc', '{"user":"alice","msg":"b"}'),
	(50*1000,  'svc', '{"user":"bob","msg":"c"}'),

	(70*1000,  'svc', '{"user":"alice","msg":"d"}'),
	(95*1000,  'svc', '{"user":"carol","msg":"e"}');`)

	q := `sum by (user)(count_over_time({job="svc"} | json [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows; sql=\n%s", sql)
	}

	type key struct {
		bucket int64
		user   string
	}
	got := map[key]int64{}
	for _, r := range rows {
		b := i64(r["bucket_ts"])
		u := s(r["user"])
		sum := i64(r["sum"])
		got[key{b, u}] = sum
	}

	exp := map[key]int64{
		{0, "alice"}:     2,
		{0, "bob"}:       1,
		{60000, "alice"}: 1,
		{60000, "carol"}: 1,
	}
	for k, want := range exp {
		if got[k] != want {
			t.Fatalf("unexpected sum for %v: got=%v want=%v; rows=%v\nsql=\n%s", k, got[k], want, rows, sql)
		}
	}
}

// --- regexp -----------------------------------------------------------------
// Group by a named capture created by regexp: (?P<pod>...)
func TestRewrite_SumBy_Regexp_Pod(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Messages embed "pod=<name>"
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', 'pod=api-7f src=a'),
	(20*1000,  'svc', 'pod=api-7f src=b'),
	(40*1000,  'svc', 'pod=api-9x src=b'),

	(70*1000,  'svc', 'pod=api-7f src=a'),
	(90*1000,  'svc', 'pod=api-9x src=b');`)

	// Named capture "pod"
	q := `sum by (pod)(count_over_time({job="svc"} | regexp "pod=(?P<pod>[^\\s]+)" [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows; sql=\n%s", sql)
	}

	type key struct {
		bucket int64
		pod    string
	}
	got := map[key]int64{}
	for _, r := range rows {
		b := i64(r["bucket_ts"])
		pod := s(r["pod"])
		sum := i64(r["sum"])
		got[key{b, pod}] = sum
	}

	exp := map[key]int64{
		{0, "api-7f"}:     2,
		{0, "api-9x"}:     1,
		{60000, "api-7f"}: 1,
		{60000, "api-9x"}: 1,
	}
	for k, want := range exp {
		if got[k] != want {
			t.Fatalf("unexpected sum for %v: got=%v want=%v; rows=%v\nsql=\n%s", k, got[k], want, rows, sql)
		}
	}
}

// --- logfmt (pattern-like) --------------------------------------------------
// Group by a key parsed from logfmt: svc=<name>
func TestRewrite_SumBy_Logfmt_Svc(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// Messages like: "svc=api user=alice" etc.
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', 'svc=api user=a'),
	(25*1000,  'svc', 'svc=api user=b'),
	(45*1000,  'svc', 'svc=auth user=c'),

	(70*1000,  'svc', 'svc=api user=d'),
	(95*1000,  'svc', 'svc=auth user=e');`)

	q := `sum by (svc)(count_over_time({job="svc"} | logfmt [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)
	if len(rows) == 0 {
		t.Fatalf("no rows; sql=\n%s", sql)
	}

	type key struct {
		bucket int64
		svc    string
	}
	got := map[key]int64{}
	for _, r := range rows {
		b := i64(r["bucket_ts"])
		svc := s(r["svc"])
		sum := i64(r["sum"])
		got[key{b, svc}] = sum
	}

	exp := map[key]int64{
		{0, "api"}:      2,
		{0, "auth"}:     1,
		{60000, "api"}:  1,
		{60000, "auth"}: 1,
	}
	for k, want := range exp {
		if got[k] != want {
			t.Fatalf("unexpected sum for %v: got=%v want=%v; rows=%v\nsql=\n%s", k, got[k], want, rows, sql)
		}
	}
}

//  1. Group-by a BASE label (job) while a parser is present and a parser-created
//     label is used in a filter (json + user="alice")
func TestRewrite_CountOverTime_ByBaseJob_WithJSONFilter(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// bucket 0: (svc, alice), (svc, bob), (api, alice)
	// bucket 1: (svc, alice), (api, carol)
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"user":"alice"}'),
	(20*1000,  'svc', '{"user":"bob"}'),
	(30*1000,  'api', '{"user":"alice"}'),
	(70*1000,  'svc', '{"user":"alice"}'),
	(90*1000,  'api', '{"user":"carol"}');`)

	// Keep only rows with user="alice" (parser-created), but group by base label "job".
	q := `sum by (job) (count_over_time({job=~".+"} | json | user="alice" [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)

	type key struct {
		bucket int64
		job    string
	}
	got := map[key]int64{}
	for _, r := range rows {
		got[key{
			bucket: i64(r["bucket_ts"]),
			job:    s(r["job"]),
		}] = i64(r["sum"])
	}

	exp := map[key]int64{
		{0, "svc"}:     1, // (10s, svc, alice)
		{0, "api"}:     1, // (30s, api, alice)
		{60000, "svc"}: 1, // (70s, svc, alice)
	}
	for k, want := range exp {
		if got[k] != want {
			t.Fatalf("unexpected sum for %v: got=%v want=%v; rows=%v\nsql=\n%s", k, got[k], want, rows, sql)
		}
	}
}

//  2. Matcher on a parser-created label (json), grouped by that same label.
//     Only rows with user="alice" should remain.
func TestRewrite_CountOverTime_ByUser_JSON_WithParserMatcher(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// bucket 0: alice, bob, alice  -> alice x2
	// bucket 1: alice, bob         -> alice x1
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', '{"user":"alice"}'),
	(20*1000,  'svc', '{"user":"bob"}'),
	(40*1000,  'svc', '{"user":"alice"}'),
	(70*1000,  'svc', '{"user":"alice"}'),
	(90*1000,  'svc', '{"user":"bob"}');`)

	q := `sum by (user) (count_over_time({job="svc"} | json | user="alice" [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)

	type key struct {
		bucket int64
		user   string
	}
	got := map[key]int64{}
	for _, r := range rows {
		got[key{
			bucket: i64(r["bucket_ts"]),
			user:   s(r["user"]),
		}] = i64(r["sum"])
	}

	exp := map[key]int64{
		{0, "alice"}:     2,
		{60000, "alice"}: 1,
	}
	for k, want := range exp {
		if got[k] != want {
			t.Fatalf("unexpected sum for %v: got=%v want=%v; rows=%v\nsql=\n%s", k, got[k], want, rows, sql)
		}
	}
}

//  3. Multiple group keys: sum by (user, svc) with logfmt parser.
//     Validates projection & GROUP BY for multi-key aggregation.
func TestRewrite_CountOverTime_ByUserSvc_Logfmt_MultiGroup(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		job TEXT,
		"_cardinalhq.message"     TEXT
	);`)

	// bucket 0:
	//   svc=api  user=a
	//   svc=api  user=b
	//   svc=auth user=a
	// bucket 1:
	//   svc=api  user=a
	//   svc=auth user=b
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp", job, "_cardinalhq.message") VALUES
	(10*1000,  'svc', 'svc=api user=a'),
	(25*1000,  'svc', 'svc=api user=b'),
	(50*1000,  'svc', 'svc=auth user=a'),
	(70*1000,  'svc', 'svc=api user=a'),
	(95*1000,  'svc', 'svc=auth user=b');`)

	q := `sum by (user, svc) (count_over_time({job="svc"} | logfmt [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)
	rows := queryAll(t, db, sql)

	type key struct {
		bucket int64
		user   string
		svc    string
	}
	got := map[key]int64{}
	for _, r := range rows {
		got[key{
			bucket: i64(r["bucket_ts"]),
			user:   s(r["user"]),
			svc:    s(r["svc"]),
		}] = i64(r["sum"])
	}

	exp := map[key]int64{
		{0, "a", "api"}:      1,
		{0, "b", "api"}:      1,
		{0, "a", "auth"}:     1,
		{60000, "a", "api"}:  1,
		{60000, "b", "auth"}: 1,
	}
	for k, want := range exp {
		if got[k] != want {
			t.Fatalf("unexpected sum for %v: got=%v want=%v; rows=%v\nsql=\n%s", k, got[k], want, rows, sql)
		}
	}
}

//  4. Group by a BASE column (resource.k8s.pod.name) while json is present and used
//     in a filter. With the "mark all non-base groupKeys as parser-created" assumption,
//     s0 won't project the pod column and the query will fail to bind.
//     Once fixed (only mark truly parser-created keys), this should pass.
func TestRewrite_CountOverTime_ByBasePod_WithJSONFilter(t *testing.T) {
	db := openDuckDB(t)
	mustExec(t, db, `CREATE TABLE logs(
		"_cardinalhq.timestamp"   BIGINT,
		"_cardinalhq.id"          TEXT,
		"_cardinalhq.level"       TEXT,
		"_cardinalhq.fingerprint" TEXT,
		"_cardinalhq.message"     TEXT,
		"resource.service.name"   TEXT,
		"resource.k8s.pod.name"   TEXT
	);`)

	// Two 1m buckets: [0..60s) and [60..120s)
	// bucket 0: (pod=api-7f, alice), (pod=api-7f, bob), (pod=api-9x, alice)
	// bucket 1: (pod=api-7f, alice), (pod=api-9x, bob)
	mustExec(t, db, `
	INSERT INTO logs("_cardinalhq.timestamp","_cardinalhq.message","resource.service.name","resource.k8s.pod.name") VALUES
	(10*1000,  '{"user":"alice"}', 'api-gateway', 'api-7f'),
	(20*1000,  '{"user":"bob"}',   'api-gateway', 'api-7f'),
	(40*1000,  '{"user":"alice"}', 'api-gateway', 'api-9x'),
	(70*1000,  '{"user":"alice"}', 'api-gateway', 'api-7f'),
	(90*1000,  '{"user":"bob"}',   'api-gateway', 'api-9x');`)

	// Keep only rows with user="alice" (parser-created), but group by the BASE pod label.
	// IMPORTANT: matcher only references resource_service_name (so pod is NOT pulled in via matchers).
	q := `sum by (resource_k8s_pod_name) (count_over_time({resource_service_name="api-gateway"} | json | user="alice" [1m]))`

	plan, _ := compileMetricPlanFromLogQL(t, q)
	if len(plan.Leaves) != 1 {
		t.Fatalf("expected 1 leaf in compiled plan, got %d", len(plan.Leaves))
	}
	be := plan.Leaves[0]

	sql := replaceWorkerPlaceholders(be.ToWorkerSQL(time.Minute), 0, 120*1000)

	// This call will currently FAIL with a DuckDB binder error because the column
	// "resource.k8s.pod.name" (or its underscored alias) isn't projected in s0.
	// After fixing the assumption (only mark parser-created keys as such), it will pass.
	rows := queryAll(t, db, sql)

	type key struct {
		bucket int64
		pod    string
	}
	got := map[key]int64{}
	for _, r := range rows {
		// Allow either dotted or underscored naming depending on rewrite
		pod := ""
		if v, ok := r[`resource.k8s.pod.name`]; ok && v != nil {
			pod = fmt.Sprint(v)
		} else if v, ok := r[`resource_k8s_pod_name`]; ok && v != nil {
			pod = fmt.Sprint(v)
		} else {
			t.Fatalf("missing pod label column in row: %v\nsql=\n%s", r, sql)
		}

		got[key{
			bucket: i64(r["bucket_ts"]),
			pod:    pod,
		}] = i64(r["sum"])
	}

	exp := map[key]int64{
		{0, "api-7f"}:     1, // (10s, alice)
		{0, "api-9x"}:     1, // (40s, alice)
		{60000, "api-7f"}: 1, // (70s, alice)
	}
	for k, want := range exp {
		if got[k] != want {
			t.Fatalf("unexpected sum for %v: got=%v want=%v; rows=%v\nsql=\n%s", k, got[k], want, rows, sql)
		}
	}
}
